{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : generaliser à images 32x32, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32\n",
    "from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def calc_U(shape, h, h_max): #dim_i, dim_j):\n",
    "    dim_i, dim_j = calc_dim(shape, h, h_max)\n",
    "    U = []\n",
    "    for i in range(dim_i):\n",
    "        for j in range(dim_j):\n",
    "            U += [(i, j)]\n",
    "    return U'''\n",
    "from waveimage import calc_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15)]\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print calc_U((32,32), 5, 6)\n",
    "print len(calc_U((32,32), 5, 6))\n",
    "print 16 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist.train.images[0]\n",
    "image = mnist_reshape_32(x, i_offset = -10, j_offset = -10)\n",
    "waveImage = WaveImage(image = image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creation de la base d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_tensor_data(batch_x):\n",
    "    batch_size, _ = batch_x.shape\n",
    "    wave_tensor = {}\n",
    "    for h in range(6):\n",
    "        if h == 0:\n",
    "            h_size = 1\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 1))\n",
    "        else:\n",
    "            h_size = 2**(h - 1)\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 3))\n",
    "    for num_batch in range(batch_size):\n",
    "        image = mnist_reshape_32(batch_x[num_batch])\n",
    "        w = WaveImage(image = image)\n",
    "        for h in range(w.get_h_max()):\n",
    "            data_h = w.get_data()[h]\n",
    "            if h == 0:\n",
    "                wave_tensor[h][num_batch][0][0][0] = data_h[(0,0)]\n",
    "            else:\n",
    "                for u in data_h:\n",
    "                    wave_tensor[h][num_batch][u[0]][u[1]][:] = data_h[u]\n",
    "    return wave_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pow2(i_ref):\n",
    "    pow2_i = np.zeros(5, dtype='int')\n",
    "    reste = i_ref\n",
    "    for p in range(4,-1,-1):\n",
    "        pow2_i[p] = int(reste // 2**p)\n",
    "        #reste = reste % 2**p\n",
    "    return pow2_i[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  7 15]\n"
     ]
    }
   ],
   "source": [
    "print calc_pow2(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wave_tensor(batch_size):\n",
    "    wave_tensor = {}\n",
    "    for h in range(6):\n",
    "        if h == 0:\n",
    "            h_size = 1\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 1))\n",
    "        else:\n",
    "            h_size = 2**(h - 1)\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 3))\n",
    "    return wave_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_tensor_data_backbone(batch_x, depth = -1, i_ref = -1, j_ref = -1):\n",
    "    tab_depth = [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6]\n",
    "    batch_size, _ = batch_x.shape\n",
    "    FLAG_RAND_I = i_ref == -1\n",
    "    FLAG_RAND_J = j_ref == -1\n",
    "    FLAG_DEPTH = depth == -1\n",
    "    wave_tensor = init_wave_tensor(batch_size)\n",
    "    for num_batch in range(batch_size):\n",
    "        image = mnist_reshape_32(batch_x[num_batch])\n",
    "        w = WaveImage(image = image)\n",
    "        if FLAG_RAND_I:\n",
    "            i_ref = np.random.randint(16)\n",
    "        if FLAG_RAND_J:\n",
    "            j_ref = np.random.randint(16)  \n",
    "        if FLAG_DEPTH:\n",
    "            indice_depth = np.random.randint(21)\n",
    "            depth = tab_depth[indice_depth]  #1 + np.random.randint(6)\n",
    "        pow2_i = calc_pow2(i_ref)\n",
    "        pow2_j = calc_pow2(j_ref)\n",
    "        for h in range(6 - depth, 6):\n",
    "            data_h = w.get_data()[h]\n",
    "            if h == 0:\n",
    "                wave_tensor[h][num_batch][0][0][0] = data_h[(0,0)] #/ 4**4\n",
    "            else:\n",
    "                u = (pow2_i[h - 1], pow2_j[h - 1])\n",
    "                #for u in data_h:\n",
    "                #    wave_tensor[h][num_batch][u[0]][u[1]][:] = 0\n",
    "                wave_tensor[h][num_batch][u[0]][u[1]][:] = data_h[u] #/ 4 ** (5 - h)\n",
    "    return wave_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Construction du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Obj()\n",
    "params.batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mnist.train.next_batch(params.batch_size)\n",
    "wave_tensor = wave_tensor_data(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction \n",
    "+ 5 couches convolutionnelles : 16 x 16 --> 8 x 8 ; 8 x 8 --> 4 x 4 etc\n",
    "+ 1 couche FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_WAV = 3\n",
    "\n",
    "DIM_5 = 16\n",
    "WIDTH = 2\n",
    "\n",
    "DEPTH_4 = 32\n",
    "DIM_4 = DIM_5 / WIDTH # 8\n",
    "\n",
    "DEPTH_3 = 64\n",
    "DIM_3 = DIM_4 / WIDTH # 4\n",
    "\n",
    "DEPTH_2 = 128\n",
    "DIM_2 = DIM_3 / WIDTH # 2\n",
    "\n",
    "DEPTH_1 = 256\n",
    "DIM_1 = DIM_2 / WIDTH # 1\n",
    "\n",
    "DIM_HIDDEN = 512\n",
    "\n",
    "NB_LABEL = 10\n",
    "\n",
    "STD = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 8192 32768 131072 133120 5120\n"
     ]
    }
   ],
   "source": [
    "nb_param_54 = (DEPTH_WAV * WIDTH * WIDTH) * DEPTH_4\n",
    "nb_param_43 = (DEPTH_4 * WIDTH * WIDTH) * DEPTH_3\n",
    "nb_param_32 = (DEPTH_3 * WIDTH * WIDTH) * DEPTH_2\n",
    "nb_param_21 = (DEPTH_2 * WIDTH * WIDTH) * DEPTH_1\n",
    "nb_param_1h = (DEPTH_1 + DEPTH_WAV + 1) * DIM_HIDDEN\n",
    "nb_param_hr = DIM_HIDDEN * NB_LABEL\n",
    "print nb_param_54, nb_param_43, nb_param_32, nb_param_21, nb_param_1h, nb_param_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev = 0.1, name = \"dummy\"):\n",
    "    #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    initial = tf.random_normal(shape, stddev = stddev)\n",
    "    #initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5 = tf.placeholder(tf.float32, shape=[None, DIM_5, DIM_5, DEPTH_WAV])\n",
    "x_4 = tf.placeholder(tf.float32, shape=[None, DIM_4, DIM_4, DEPTH_WAV])\n",
    "x_3 = tf.placeholder(tf.float32, shape=[None, DIM_3, DIM_3, DEPTH_WAV])\n",
    "x_2 = tf.placeholder(tf.float32, shape=[None, DIM_2, DIM_2, DEPTH_WAV])\n",
    "x_1 = tf.placeholder(tf.float32, shape=[None, DIM_1, DIM_1, DEPTH_WAV])\n",
    "x_0 = tf.placeholder(tf.float32, shape=[None, 1, 1, 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_phase = tf.placeholder(tf.bool, name='bn_phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 --> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "W_conv_54 = weight_variable([WIDTH, WIDTH, DEPTH_WAV, DEPTH_4], \\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_WAV), \\\n",
    "                            name = \"W_conv_54\")\n",
    "# Graph construction\n",
    "h_conv_4 = tf.nn.conv2d(x_5, W_conv_54, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_4') \n",
    "#h_pool_4 = tf.nn.max_pool(h_conv_4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_4')\n",
    "#h_pool_4_bn = tf.contrib.layers.batch_norm(h_pool_4, center=True, scale=True, is_training=batch_phase, scope='h_pool_4', updates_collections=None)\n",
    "z_conv_4 = tf.nn.relu(h_conv_4)\n",
    "\n",
    "#h_conv_4 = tf.nn.conv2d(x_5, W_conv_54, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_4') \n",
    "#h_conv_4_bn = tf.contrib.layers.batch_norm(h_conv_4, center=True, scale=True, is_training=batch_phase, scope='h_conv_4', updates_collections=None)\n",
    "#z_conv_4 = tf.nn.relu(h_conv_4_bn)\n",
    "\n",
    "cat_conv_4 = tf.concat((z_conv_4, x_4), axis = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 --> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "W_conv_43 = weight_variable([WIDTH, WIDTH, DEPTH_4 + DEPTH_WAV, DEPTH_3],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * (DEPTH_4 + DEPTH_WAV)), \\\n",
    "                            name = \"W_conv_43\")\n",
    "\n",
    "h_conv_3 = tf.nn.conv2d(cat_conv_4, W_conv_43, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_3') \n",
    "#h_pool_3 = tf.nn.max_pool(h_conv_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_3')\n",
    "#h_pool_3_bn = tf.contrib.layers.batch_norm(h_pool_3, center=True, scale=True, is_training=batch_phase, scope='h_pool_3', updates_collections=None)\n",
    "z_conv_3 = tf.nn.relu(h_conv_3)\n",
    "\n",
    "# Graph construction\n",
    "#h_conv_3 = tf.nn.conv2d(cat_conv_4, W_conv_43, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_3') \n",
    "#h_conv_3_bn = tf.contrib.layers.batch_norm(h_conv_3, center=True, scale=True, is_training=batch_phase, scope='h_conv_3', updates_collections=None)\n",
    "#z_conv_3 = tf.nn.relu(h_conv_3_bn)\n",
    "\n",
    "cat_conv_3 = tf.concat((z_conv_3, x_3), axis = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 --> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "W_conv_32 = weight_variable([WIDTH, WIDTH, DEPTH_3 + DEPTH_WAV, DEPTH_2],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * (DEPTH_3 + DEPTH_WAV)), \\\n",
    "                            name = \"W_conv_32\")\n",
    "\n",
    "# Graph construction\n",
    "h_conv_2 = tf.nn.conv2d(cat_conv_3, W_conv_32, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_2') \n",
    "#h_pool_2 = tf.nn.max_pool(h_conv_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_2')\n",
    "#h_pool_2_bn = tf.contrib.layers.batch_norm(h_pool_2, center=True, scale=True, is_training=batch_phase, scope='h_pool_2', updates_collections=None)\n",
    "z_conv_2 = tf.nn.relu(h_conv_2)\n",
    "\n",
    "#h_conv_2 = tf.nn.conv2d(cat_conv_3, W_conv_32, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_2') \n",
    "#h_conv_2_bn = tf.contrib.layers.batch_norm(h_conv_2, center=True, scale=True, is_training=batch_phase, scope='h_conv_2', updates_collections=None)\n",
    "#z_conv_2 = tf.nn.relu(h_conv_2_bn)\n",
    "\n",
    "cat_conv_2 = tf.concat((z_conv_2, x_2), axis = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "W_conv_21 = weight_variable([WIDTH, WIDTH, DEPTH_2 + DEPTH_WAV, DEPTH_1],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * (DEPTH_2 + DEPTH_WAV)), \\\n",
    "                            name = \"W_conv_21\")\n",
    "\n",
    "h_conv_1 = tf.nn.conv2d(cat_conv_2, W_conv_21, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_1') \n",
    "#h_pool_1 = tf.nn.max_pool(h_conv_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_1')\n",
    "#h_pool_1_bn = tf.contrib.layers.batch_norm(h_pool_1, center=True, scale=True, is_training=batch_phase, scope='h_pool_1', updates_collections=None)\n",
    "z_conv_1 = tf.nn.relu(h_conv_1)\n",
    "\n",
    "# Graph construction\n",
    "#h_conv_1 = tf.nn.conv2d(cat_conv_2, W_conv_21, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_1') \n",
    "#h_conv_1_bn = tf.contrib.layers.batch_norm(h_conv_1, center=True, scale=True, is_training=batch_phase, scope='h_conv_1', updates_collections=None)\n",
    "#z_conv_1 = tf.nn.relu(h_conv_1_bn)\n",
    "\n",
    "cat_conv_1 = tf.concat((z_conv_1, x_1, x_0), axis = 3)\n",
    "z_flat1 = tf.reshape(cat_conv_1, [-1, DEPTH_1 + DEPTH_WAV + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hidden = weight_variable([DEPTH_1 + DEPTH_WAV + 1, DIM_HIDDEN], stddev = STD / (DEPTH_1 + DEPTH_WAV + 1), name = \"W_hidden\")\n",
    "h_hidden = tf.matmul(z_flat1, W_hidden)\n",
    "z_hidden = tf.nn.relu(h_hidden)\n",
    "z_hidden_drop = tf.nn.dropout(z_hidden, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_readout = weight_variable([DIM_HIDDEN, NB_LABEL], stddev = STD / DIM_HIDDEN, name = \"W_readout\")\n",
    "y_hat_logit = tf.matmul(z_hidden_drop, W_readout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss graph¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat_logit))\n",
    "\n",
    "l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "   scale=0.005, scope=None\n",
    ")\n",
    "weights = tf.trainable_variables() # all vars of your graph\n",
    "regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "\n",
    "regularized_loss = classif_loss #+ regularization_penalty # this loss needs to be min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train graph¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer(1e-4).minimize(regularized_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_hat_logit, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Obj()\n",
    "mem.num_epoch = []\n",
    "mem.classif_eval = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.n_epochs = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 392480\t classif : 0.46000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f3b18702492d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_mem.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mwave_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave_tensor_data_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mx_4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mx_3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mx_2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mx_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mx_0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwave_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mbatch_phase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-625aaea97866>\u001b[0m in \u001b[0;36mwave_tensor_data_backbone\u001b[0;34m(batch_x, depth, i_ref, j_ref)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_reshape_32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWaveImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAG_RAND_I\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mi_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dauce/Recherche/scripts/ProjS8-git/waveimage.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image, shape)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                                                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# image is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5150\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5151\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_name = \"models/mnist-waveimage-CNN-backbone-512-rnd\"\n",
    "\n",
    "if not os.path.isfile(file_name + \".ckpt.index\"):\n",
    "    for num_epoch in range (params.n_epochs):\n",
    "        if num_epoch % 10 == 0:\n",
    "            mem.num_epoch += [num_epoch]\n",
    "            x_test, y_test = mnist.test.next_batch(params.batch_size)\n",
    "            #wave_tensor = wave_tensor_data(x_test)\n",
    "            wave_tensor = wave_tensor_data_backbone(x_test)\n",
    "            classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                                    x_4: wave_tensor[4],\\\n",
    "                                                    x_3: wave_tensor[3],\\\n",
    "                                                    x_2: wave_tensor[2],\\\n",
    "                                                    x_1: wave_tensor[1],\\\n",
    "                                                    x_0: wave_tensor[0],\\\n",
    "                                                    y: y_test,\\\n",
    "                                                    keep_prob: 1,\\\n",
    "                                                    batch_phase:False})\n",
    "            mem.classif_eval += [classif_eval]\n",
    "            sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                             % (num_epoch, \\\n",
    "                                classif_eval))\n",
    "        if num_epoch % 999 == 0:\n",
    "            saver.save(sess,          file_name + \".ckpt\")\n",
    "            pickle.dump(mem,     open(file_name + \"_mem.pkl\", \"wb\"))\n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) \n",
    "        wave_tensor = wave_tensor_data_backbone(batch_x)\n",
    "        train.run(feed_dict={x_5: wave_tensor[5],\\\n",
    "                              x_4: wave_tensor[4],\\\n",
    "                              x_3: wave_tensor[3],\\\n",
    "                              x_2: wave_tensor[2],\\\n",
    "                              x_1: wave_tensor[1],\\\n",
    "                              x_0: wave_tensor[0],\\\n",
    "                              y: batch_y,\\\n",
    "                              keep_prob: 1,\\\n",
    "                              batch_phase:True})\n",
    "else:\n",
    "    saver.restore(sess,       file_name + \".ckpt\")\n",
    "    mem    = pickle.load(open(file_name + \"_mem.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
