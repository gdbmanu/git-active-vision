{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : generaliser à images 32x32, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32\n",
    "from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def calc_U(shape, h, h_max): #dim_i, dim_j):\n",
    "    dim_i, dim_j = calc_dim(shape, h, h_max)\n",
    "    U = []\n",
    "    for i in range(dim_i):\n",
    "        for j in range(dim_j):\n",
    "            U += [(i, j)]\n",
    "    return U'''\n",
    "from waveimage import calc_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_reshape_32_buf(x):\n",
    "    assert x.shape == (28 * 28,)\n",
    "    image = x.reshape(28,28)\n",
    "    image = np.append(np.zeros((2,28)), image, axis = 0)\n",
    "    image = np.append(image, np.zeros((2,28)), axis = 0)\n",
    "    image = np.append(np.zeros((32,2)), image, axis = 1)\n",
    "    image = np.append(image, np.zeros((32,2)), axis = 1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creation de la base d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_tensor_data(batch_x):\n",
    "    batch_size, _ = batch_x.shape\n",
    "    wave_tensor = {}\n",
    "    for h in range(6):\n",
    "        if h == 0:\n",
    "            h_size = 1\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 1))\n",
    "        else:\n",
    "            h_size = 2**(h - 1)\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 3))\n",
    "    for num_batch in range(batch_size):\n",
    "        image = mnist_reshape_32(batch_x[num_batch])\n",
    "        w = WaveImage(image = image)\n",
    "        for h in range(w.get_h_max()):\n",
    "            data_h = w.get_data()[h]\n",
    "            if h == 0:\n",
    "                wave_tensor[h][num_batch][0][0][0] = data_h[(0,0)]\n",
    "            else:\n",
    "                for u in data_h:\n",
    "                    wave_tensor[h][num_batch][u[0]][u[1]][:] = data_h[u]\n",
    "    return wave_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Construction du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Obj()\n",
    "params.batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mnist.train.next_batch(params.batch_size)\n",
    "wave_tensor = wave_tensor_data(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction \n",
    "+ 5 couches convolutionnelles : 16 x 16 --> 8 x 8 ; 8 x 8 --> 4 x 4 etc\n",
    "+ 1 couche FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_WAV = 3\n",
    "\n",
    "DIM_5 = 16\n",
    "WIDTH = 2\n",
    "\n",
    "DEPTH_4 = 32\n",
    "DIM_4 = DIM_5 / WIDTH # 8\n",
    "\n",
    "DEPTH_3 = 64\n",
    "DIM_3 = DIM_4 / WIDTH # 4\n",
    "\n",
    "DEPTH_2 = 128\n",
    "DIM_2 = DIM_3 / WIDTH # 2\n",
    "\n",
    "DEPTH_1 = 256\n",
    "DIM_1 = DIM_2 / WIDTH # 1\n",
    "\n",
    "DIM_HIDDEN = 512\n",
    "\n",
    "NB_LABEL = 10\n",
    "\n",
    "STD = 1.\n",
    "\n",
    "FLAG_POOL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 8192, 32768, 131072, 133120, 5120)\n"
     ]
    }
   ],
   "source": [
    "nb_param_54 = (DEPTH_WAV * WIDTH * WIDTH) * DEPTH_4\n",
    "nb_param_43 = (DEPTH_4 * WIDTH * WIDTH) * DEPTH_3\n",
    "nb_param_32 = (DEPTH_3 * WIDTH * WIDTH) * DEPTH_2\n",
    "nb_param_21 = (DEPTH_2 * WIDTH * WIDTH) * DEPTH_1\n",
    "nb_param_1h = (DEPTH_1 + DEPTH_WAV + 1) * DIM_HIDDEN\n",
    "nb_param_hr = DIM_HIDDEN * NB_LABEL\n",
    "print (nb_param_54, nb_param_43, nb_param_32, nb_param_21, nb_param_1h, nb_param_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPTH_1 + DEPTH_2 + DEPTH_3 + DEPTH_4 + DEPTH_WAV + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev = 0.1, name = \"dummy\", reuse = False):\n",
    "    #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    #initial = tf.zeros(shape)\n",
    "    if reuse:\n",
    "        return tf.get_variable(name)\n",
    "    else:\n",
    "        initial = tf.random_normal(shape, stddev = stddev)\n",
    "        return tf.Variable(initial, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5 = tf.placeholder(tf.float32, shape=[None, DIM_5, DIM_5, DEPTH_WAV])\n",
    "x_4 = tf.placeholder(tf.float32, shape=[None, DIM_4, DIM_4, DEPTH_WAV])\n",
    "x_3 = tf.placeholder(tf.float32, shape=[None, DIM_3, DIM_3, DEPTH_WAV])\n",
    "x_2 = tf.placeholder(tf.float32, shape=[None, DIM_2, DIM_2, DEPTH_WAV])\n",
    "x_1 = tf.placeholder(tf.float32, shape=[None, DIM_1, DIM_1, DEPTH_WAV])\n",
    "x_0 = tf.placeholder(tf.float32, shape=[None, 1, 1, 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_phase = tf.placeholder(tf.bool, name='bn_phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 --> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "if not FLAG_POOL :\n",
    "    W_conv_54_flux1 = weight_variable([WIDTH, WIDTH, DEPTH_WAV, DEPTH_4], \\\n",
    "                                stddev = STD / (WIDTH * WIDTH * DEPTH_WAV), \\\n",
    "                                name = \"W_conv_54_flux1\")\n",
    "    # Graph construction\n",
    "    h_conv_4_flux1 = tf.nn.conv2d(x_5, W_conv_54_flux1, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_4_flux1') \n",
    "    z_conv_4_flux1 = tf.nn.relu(h_conv_4_flux1)\n",
    "else:\n",
    "    W_conv_54_flux1 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_WAV, DEPTH_4], \\\n",
    "                                stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_WAV), \\\n",
    "                                name = \"W_conv_54_flux1\")\n",
    "    h_conv_4_flux1 = tf.nn.conv2d(x_5, W_conv_54_flux1, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_4_flux1') \n",
    "    h_pool_4_flux1 = tf.nn.max_pool(h_conv_4_flux1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_4_flux1')\n",
    "    #h_pool_4_bn_flux1 = tf.contrib.layers.batch_norm(h_pool_4_flux1, center=True, scale=True, is_training=batch_phase, scope='h_pool_4_flux1', updates_collections=None)\n",
    "    z_conv_4_flux1 = tf.nn.relu(h_pool_4_flux1)\n",
    "\n",
    "#h_conv_4 = tf.nn.conv2d(x_5, W_conv_54, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_4') \n",
    "#h_conv_4_bn = tf.contrib.layers.batch_norm(h_conv_4, center=True, scale=True, is_training=batch_phase, scope='h_conv_4', updates_collections=None)\n",
    "#z_conv_4 = tf.nn.relu(h_conv_4_bn)\n",
    "\n",
    "#cat_conv_4 = tf.concat((z_conv_4, x_4), axis = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 --> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "if not FLAG_POOL :\n",
    "    W_conv_43_flux1 = weight_variable([WIDTH, WIDTH, DEPTH_4, DEPTH_3],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_4), \\\n",
    "                            name = \"W_conv_43_flux1\")\n",
    "    h_conv_3_flux1 = tf.nn.conv2d(z_conv_4_flux1, W_conv_43_flux1, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_3_flux1') \n",
    "    z_conv_3_flux1 = tf.nn.relu(h_conv_3_flux1)\n",
    "    #\n",
    "    W_conv_43_flux2 = weight_variable([WIDTH, WIDTH, DEPTH_WAV, DEPTH_4],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_WAV), \\\n",
    "                            name = \"W_conv_43_flux1\")\n",
    "    h_conv_3_flux2 = tf.nn.conv2d(x_4, W_conv_43_flux2, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_3_flux2') \n",
    "    z_conv_3_flux2 = tf.nn.relu(h_conv_3_flux2)\n",
    "\n",
    "else:\n",
    "    W_conv_43_flux1 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_4, DEPTH_3],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_4), \\\n",
    "                            name = \"W_conv_43_flux1\")\n",
    "    h_conv_3_flux1 = tf.nn.conv2d(z_conv_4_flux1, W_conv_43_flux1, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_3_flux1') \n",
    "    h_pool_3_flux1 = tf.nn.max_pool(h_conv_3_flux1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_3_flux1')\n",
    "    #h_pool_3_bn_flux1 = tf.contrib.layers.batch_norm(h_pool_3_flux1, center=True, scale=True, is_training=batch_phase, scope='h_pool_3_flux1', updates_collections=None)\n",
    "    z_conv_3_flux1 = tf.nn.relu(h_pool_3_flux1)\n",
    "    #\n",
    "    W_conv_43_flux2 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_WAV, DEPTH_4],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_WAV), \\\n",
    "                            name = \"W_conv_43_flux1\")\n",
    "    h_conv_3_flux2 = tf.nn.conv2d(x_4, W_conv_43_flux2, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_3_flux2') \n",
    "    h_pool_3_flux2 = tf.nn.max_pool(h_conv_3_flux2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_3_flux2')\n",
    "    #h_pool_3_bn_flux2 = tf.contrib.layers.batch_norm(h_pool_3_flux2, center=True, scale=True, is_training=batch_phase, scope='h_pool_3_flux2', updates_collections=None)\n",
    "    z_conv_3_flux2 = tf.nn.relu(h_pool_3_flux2)\n",
    "\n",
    "# Graph construction\n",
    "#h_conv_3 = tf.nn.conv2d(cat_conv_4, W_conv_43, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_3') \n",
    "#h_conv_3_bn = tf.contrib.layers.batch_norm(h_conv_3, center=True, scale=True, is_training=batch_phase, scope='h_conv_3', updates_collections=None)\n",
    "#z_conv_3 = tf.nn.relu(h_conv_3_bn)\n",
    "\n",
    "#cat_conv_3 = tf.concat((z_conv_3, x_3), axis = 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 --> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "if not FLAG_POOL :\n",
    "    W_conv_32_flux1 = weight_variable([WIDTH, WIDTH, DEPTH_3 , DEPTH_2],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_3 ), \\\n",
    "                            name = \"W_conv_32_flux1\")\n",
    "    h_conv_2_flux1 = tf.nn.conv2d(z_conv_3_flux1, W_conv_32_flux1, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_2_flux1') \n",
    "    z_conv_2_flux1 = tf.nn.relu(h_conv_2_flux1)\n",
    "    \n",
    "    W_conv_32_flux2 = weight_variable([WIDTH, WIDTH, DEPTH_4 , DEPTH_3],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_4 ), \\\n",
    "                            name = \"W_conv_32_flux1\")\n",
    "    h_conv_2_flux2 = tf.nn.conv2d(z_conv_3_flux2, W_conv_32_flux2, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_2_flux2') \n",
    "    z_conv_2_flux2 = tf.nn.relu(h_conv_2_flux2)\n",
    "    \n",
    "    W_conv_32_flux3 = weight_variable([WIDTH, WIDTH, DEPTH_WAV , DEPTH_4],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_WAV ), \\\n",
    "                            name = \"W_conv_32_flux3\")\n",
    "    h_conv_2_flux3 = tf.nn.conv2d(x_3, W_conv_32_flux3, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_2_flux3') \n",
    "    z_conv_2_flux3 = tf.nn.relu(h_conv_2_flux3)\n",
    "\n",
    "else:\n",
    "    W_conv_32_flux1 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_3 , DEPTH_2],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_3 ), \\\n",
    "                            name = \"W_conv_32_flux1\")\n",
    "    h_conv_2_flux1 = tf.nn.conv2d(z_conv_3_flux1, W_conv_32_flux1, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_2_flux1') \n",
    "    h_pool_2_flux1 = tf.nn.max_pool(h_conv_2_flux1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_2_flux1')\n",
    "    #h_pool_2_bn_flux1 = tf.contrib.layers.batch_norm(h_pool_2_flux1, center=True, scale=True, is_training=batch_phase, scope='h_pool_2_flux1', updates_collections=None)\n",
    "    z_conv_2_flux1 = tf.nn.relu(h_pool_2_flux1)\n",
    "    \n",
    "    W_conv_32_flux2 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_4 , DEPTH_3],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_4 ), \\\n",
    "                            name = \"W_conv_32_flux1\")\n",
    "    h_conv_2_flux2 = tf.nn.conv2d(z_conv_3_flux2, W_conv_32_flux2, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_2_flux2') \n",
    "    h_pool_2_flux2 = tf.nn.max_pool(h_conv_2_flux2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_2_flux2')\n",
    "    #h_pool_2_bn_flux2 = tf.contrib.layers.batch_norm(h_pool_2_flux2, center=True, scale=True, is_training=batch_phase, scope='h_pool_2_flux2', updates_collections=None)\n",
    "    z_conv_2_flux2 = tf.nn.relu(h_pool_2_flux2)\n",
    "    \n",
    "    W_conv_32_flux3 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_WAV , DEPTH_4],\\\n",
    "                            stddev = STD / (WIDTH * WIDTH * DEPTH_WAV ), \\\n",
    "                            name = \"W_conv_32_flux3\")\n",
    "    h_conv_2_flux3 = tf.nn.conv2d(x_3, W_conv_32_flux3, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_2_flux3') \n",
    "    h_pool_2_flux3 = tf.nn.max_pool(h_conv_2_flux3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_2_flux3')\n",
    "    #h_pool_2_bn_flux3 = tf.contrib.layers.batch_norm(h_pool_2_flux3, center=True, scale=True, is_training=batch_phase, scope='h_pool_2_flux3', updates_collections=None)\n",
    "    z_conv_2_flux3 = tf.nn.relu(h_pool_2_flux3)\n",
    "\n",
    "#h_conv_2 = tf.nn.conv2d(cat_conv_3, W_conv_32, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_2') \n",
    "#h_conv_2_bn = tf.contrib.layers.batch_norm(h_conv_2, center=True, scale=True, is_training=batch_phase, scope='h_conv_2', updates_collections=None)\n",
    "#z_conv_2 = tf.nn.relu(h_conv_2_bn)\n",
    "\n",
    "#cat_conv_2 = tf.concat((z_conv_2, x_2), axis = 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FLAG_POOL :\n",
    "    W_conv_21_flux1 = weight_variable([WIDTH, WIDTH, DEPTH_2, DEPTH_1],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * DEPTH_2), \\\n",
    "                                name = \"W_conv_21_flux1\")\n",
    "    h_conv_1_flux1 = tf.nn.conv2d(z_conv_2_flux1, W_conv_21_flux1, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_1_flux1') \n",
    "    z_conv_1_flux1 = tf.nn.relu(h_conv_1_flux1)\n",
    "\n",
    "    # Graph construction\n",
    "    #h_conv_1 = tf.nn.conv2d(cat_conv_2, W_conv_21, strides=[1, WIDTH, WIDTH, 1], padding='VALID', name='h_conv_1') \n",
    "    #h_conv_1_bn = tf.contrib.layers.batch_norm(h_conv_1, center=True, scale=True, is_training=batch_phase, scope='h_conv_1', updates_collections=None)\n",
    "    #z_conv_1 = tf.nn.relu(h_conv_1_bn)\n",
    "\n",
    "    W_conv_21_flux2 = weight_variable([WIDTH, WIDTH, DEPTH_3, DEPTH_2],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * DEPTH_3), \\\n",
    "                                name = \"W_conv_21_flux2\")\n",
    "    h_conv_1_flux2 = tf.nn.conv2d(z_conv_2_flux2, W_conv_21_flux2, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_1_flux2') \n",
    "    z_conv_1_flux2 = tf.nn.relu(h_conv_1_flux2)\n",
    "    \n",
    "    W_conv_21_flux3 = weight_variable([WIDTH, WIDTH, DEPTH_4, DEPTH_3],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * DEPTH_4), \\\n",
    "                                name = \"W_conv_21_flux3\")\n",
    "    h_conv_1_flux3 = tf.nn.conv2d(z_conv_2_flux3, W_conv_21_flux3, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_1_flux2') \n",
    "    z_conv_1_flux3 = tf.nn.relu(h_conv_1_flux3)\n",
    "    \n",
    "    W_conv_21_flux4 = weight_variable([WIDTH, WIDTH, DEPTH_WAV, DEPTH_4],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * DEPTH_WAV), \\\n",
    "                                name = \"W_conv_21_flux4\")\n",
    "    h_conv_1_flux4 = tf.nn.conv2d(x_2, W_conv_21_flux4, strides=[1, 2, 2, 1], padding='VALID', name='h_conv_1_flux4') \n",
    "    z_conv_1_flux4 = tf.nn.relu(h_conv_1_flux4)\n",
    "    \n",
    "else:\n",
    "    W_conv_21_flux1 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_2, DEPTH_1],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_2), \\\n",
    "                                name = \"W_conv_21_flux1\")\n",
    "    h_conv_1_flux1 = tf.nn.conv2d(z_conv_2_flux1, W_conv_21_flux1, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_1_flux1') \n",
    "    h_pool_1_flux1 = tf.nn.max_pool(h_conv_1_flux1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_1_flux1')\n",
    "    #h_pool_1_bn_flux1 = tf.contrib.layers.batch_norm(h_pool_1_flux1, center=True, scale=True, is_training=batch_phase, scope='h_pool_1_flux1', updates_collections=None)\n",
    "    z_conv_1_flux1 = tf.nn.relu(h_pool_1_flux1)\n",
    "    \n",
    "    W_conv_21_flux2 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_3, DEPTH_2],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_3), \\\n",
    "                                name = \"W_conv_21_flux2\")\n",
    "    h_conv_1_flux2 = tf.nn.conv2d(z_conv_2_flux2, W_conv_21_flux2, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_1_flux2') \n",
    "    h_pool_1_flux2 = tf.nn.max_pool(h_conv_1_flux2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_1_flux2')\n",
    "    #h_pool_1_bn_flux2 = tf.contrib.layers.batch_norm(h_pool_1_flux2, center=True, scale=True, is_training=batch_phase, scope='h_pool_1_flux2', updates_collections=None)\n",
    "    z_conv_1_flux2 = tf.nn.relu(h_pool_1_flux2)\n",
    "    \n",
    "    W_conv_21_flux3 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_4, DEPTH_3],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_4), \\\n",
    "                                name = \"W_conv_21_flux3\")\n",
    "    h_conv_1_flux3 = tf.nn.conv2d(z_conv_2_flux3, W_conv_21_flux3, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_1_flux2') \n",
    "    h_pool_1_flux3 = tf.nn.max_pool(h_conv_1_flux3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_1_flux3')\n",
    "    #h_pool_1_bn_flux3 = tf.contrib.layers.batch_norm(h_pool_1_flux3, center=True, scale=True, is_training=batch_phase, scope='h_pool_1_flux3', updates_collections=None)\n",
    "    z_conv_1_flux3 = tf.nn.relu(h_pool_1_flux3)\n",
    "    \n",
    "    W_conv_21_flux4 = weight_variable([WIDTH * 2, WIDTH * 2, DEPTH_WAV, DEPTH_4],\\\n",
    "                                stddev = STD / (WIDTH * WIDTH * 4 * DEPTH_WAV), \\\n",
    "                                name = \"W_conv_21_flux4\")\n",
    "    h_conv_1_flux4 = tf.nn.conv2d(x_2, W_conv_21_flux4, strides=[1, 1, 1, 1], padding='SAME', name='h_conv_1_flux4') \n",
    "    h_pool_1_flux4 = tf.nn.max_pool(h_conv_1_flux4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='h_pool_1_flux4')\n",
    "    #h_pool_1_bn_flux4 = tf.contrib.layers.batch_norm(h_pool_1_flux4, center=True, scale=True, is_training=batch_phase, scope='h_pool_1_flux4', updates_collections=None)\n",
    "    z_conv_1_flux4 = tf.nn.relu(h_pool_1_flux4)\n",
    "##\n",
    "\n",
    "#cat_conv_1 = tf.concat((z_conv_1_flux1, z_conv_1_flux2, z_conv_1_flux3, z_conv_1_flux4, x_1, x_0), axis = 3)\n",
    "#z_flat1 = tf.reshape(cat_conv_1, [-1, DEPTH_1 + DEPTH_2 + DEPTH_3 + DEPTH_4 + DEPTH_WAV + 1])#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_conv_1_flux5 = tf.concat((x_1, x_0), axis = 3)\n",
    "W_hidden_flux5 = weight_variable([DEPTH_WAV + 1, DEPTH_4], stddev = STD / (DEPTH_WAV + 1), name = \"W_hidden_flux5\")\n",
    "h_hidden_flux5 = tf.matmul(tf.reshape(z_conv_1_flux5, [-1, DEPTH_WAV + 1]), W_hidden_flux5)\n",
    "z_hidden_flux5 = tf.nn.relu(h_hidden_flux5)\n",
    "\n",
    "z_concat_4 = tf.concat((tf.reshape(z_conv_1_flux4, [-1, DEPTH_4]), z_hidden_flux5), axis = 1)\n",
    "W_hidden_flux4 = weight_variable([DEPTH_4 + DEPTH_4, DEPTH_3], stddev = STD / (DEPTH_4 + DEPTH_4), name = \"W_hidden_flux2\")\n",
    "h_hidden_flux4 = tf.matmul(z_concat_4, W_hidden_flux4)\n",
    "z_hidden_flux4 = tf.nn.relu(h_hidden_flux4)\n",
    "\n",
    "z_concat_3 = tf.concat((tf.reshape(z_conv_1_flux3, [-1, DEPTH_3]), z_hidden_flux4), axis = 1)\n",
    "W_hidden_flux3 = weight_variable([DEPTH_3 + DEPTH_3, DEPTH_2], stddev = STD / (DEPTH_3 + DEPTH_3), name = \"W_hidden_flux3\")\n",
    "h_hidden_flux3 = tf.matmul(z_concat_3, W_hidden_flux3)\n",
    "z_hidden_flux3 = tf.nn.relu(h_hidden_flux3)\n",
    "\n",
    "z_concat_2 = tf.concat((tf.reshape(z_conv_1_flux2, [-1, DEPTH_2]), z_hidden_flux3), axis = 1)\n",
    "W_hidden_flux2 = weight_variable([DEPTH_2 + DEPTH_2, DEPTH_1], stddev = STD / (DEPTH_2 + DEPTH_2), name = \"W_hidden_flux2\")\n",
    "h_hidden_flux2 = tf.matmul(z_concat_2, W_hidden_flux2)\n",
    "z_hidden_flux2 = tf.nn.relu(h_hidden_flux2)\n",
    "\n",
    "z_concat_1 = tf.concat((tf.reshape(z_conv_1_flux1, [-1, DEPTH_1]), z_hidden_flux2), axis = 1)\n",
    "W_hidden_flux1 = weight_variable([DEPTH_1 + DEPTH_1, DIM_HIDDEN], stddev = STD / (DEPTH_1 + DEPTH_1), name = \"W_hidden_flux1\")\n",
    "h_hidden_flux1 = tf.matmul(z_concat_1, W_hidden_flux1)\n",
    "z_hidden_flux1 = tf.nn.relu(h_hidden_flux1)\n",
    "\n",
    "#z_hidden_concat = tf.concat((z_hidden_flux1, z_hidden_flux2, z_hidden_flux3, z_hidden_flux4, z_hidden_flux5), axis = 1)\n",
    "W_hidden = weight_variable([DIM_HIDDEN, DIM_HIDDEN], stddev = STD / DIM_HIDDEN, name = \"W_hidden\")\n",
    "h_hidden = tf.matmul(z_hidden_flux1, W_hidden)\n",
    "z_hidden = tf.nn.relu(h_hidden)\n",
    "z_hidden_drop = tf.nn.dropout(z_hidden, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W_hidden = weight_variable([DEPTH_1 + DEPTH_2 + DEPTH_3 + DEPTH_4 + DEPTH_WAV + 1, DIM_HIDDEN], stddev = STD / (DEPTH_1 + DEPTH_2 + DEPTH_3 + DEPTH_4 + DEPTH_WAV + 1), name = \"W_hidden\")\n",
    "#h_hidden = tf.matmul(z_flat1, W_hidden)\n",
    "#z_hidden = tf.nn.relu(h_hidden)\n",
    "#z_hidden_drop = tf.nn.dropout(z_hidden, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_readout = weight_variable([DIM_HIDDEN, NB_LABEL], stddev = STD / DIM_HIDDEN, name = \"W_readout\")\n",
    "y_hat_logit = tf.matmul(z_hidden_drop, W_readout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss graph¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat_logit))\n",
    "\n",
    "#l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "#   scale=0.005, scope=None\n",
    "#)\n",
    "#weights = tf.trainable_variables() # all vars of your graph\n",
    "#regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "\n",
    "regularized_loss = classif_loss #+ regularization_penalty # this loss needs to be min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train graph¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer(1e-5).minimize(regularized_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_hat_logit, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Obj()\n",
    "mem.num_epoch = []\n",
    "mem.classif_eval = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.n_epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1640\t classif : 0.98000"
     ]
    }
   ],
   "source": [
    "file_name = \"models/mnist-waveimage-CNN-parallel-comb-alt-500\"\n",
    "\n",
    "if not os.path.isfile(file_name + \".ckpt.index\"):\n",
    "    for num_epoch in range (params.n_epochs):\n",
    "        if num_epoch % 10 == 0:\n",
    "            mem.num_epoch += [num_epoch]\n",
    "            x_test, y_test = mnist.test.next_batch(params.batch_size)\n",
    "            wave_tensor = wave_tensor_data(x_test)\n",
    "            classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                                    x_4: wave_tensor[4],\\\n",
    "                                                    x_3: wave_tensor[3],\\\n",
    "                                                    x_2: wave_tensor[2],\\\n",
    "                                                    x_1: wave_tensor[1],\\\n",
    "                                                    x_0: wave_tensor[0],\\\n",
    "                                                    y: y_test,\\\n",
    "                                                    keep_prob: 1,\\\n",
    "                                                    batch_phase:False})\n",
    "            mem.classif_eval += [classif_eval]\n",
    "            sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                             % (num_epoch, \\\n",
    "                                classif_eval))\n",
    "        if num_epoch % 1000 == 0:\n",
    "            saver.save(sess,          file_name + \".ckpt\")\n",
    "            pickle.dump(mem,     open(file_name + \"_mem.pkl\", \"wb\"))\n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) \n",
    "        wave_tensor = wave_tensor_data(batch_x)\n",
    "        train.run(feed_dict={x_5: wave_tensor[5],\\\n",
    "                              x_4: wave_tensor[4],\\\n",
    "                              x_3: wave_tensor[3],\\\n",
    "                              x_2: wave_tensor[2],\\\n",
    "                              x_1: wave_tensor[1],\\\n",
    "                              x_0: wave_tensor[0],\\\n",
    "                              y: batch_y,\\\n",
    "                              keep_prob: 0.5,\\\n",
    "                              batch_phase:True})\n",
    "else:\n",
    "    saver.restore(sess, file_name + \".ckpt\")\n",
    "    mem = pickle.load(open(file_name + \"_mem.pkl\", \"rb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95313ec790>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu4hIL6RwFxHphRTuIiK9kMJdRKQX+l+rbckdXprSLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f953156cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mem.classif_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "test_tensor = {}\n",
    "test_tensor[5] = np.zeros((1, DIM_5, DIM_5, DEPTH_WAV))\n",
    "test_tensor[4] = np.zeros((1, DIM_4, DIM_4, DEPTH_WAV))\n",
    "test_tensor[3] = np.zeros((1, DIM_3, DIM_3, DEPTH_WAV))\n",
    "test_tensor[2] = np.zeros((1, DIM_2, DIM_2, DEPTH_WAV))\n",
    "test_tensor[1] = np.zeros((1, DIM_1, DIM_1, DEPTH_WAV))\n",
    "test_tensor[0] = np.zeros((1, 1, 1, 1))\n",
    "\n",
    "test = y_hat_logit.eval(feed_dict={ x_5: test_tensor[5],\\\n",
    "                                    x_4: test_tensor[4],\\\n",
    "                                    x_3: test_tensor[3],\\\n",
    "                                    x_2: test_tensor[2],\\\n",
    "                                    x_1: test_tensor[1],\\\n",
    "                                    x_0: test_tensor[0],\\\n",
    "                                    keep_prob: 1,\\\n",
    "                                    batch_phase:False})\n",
    "\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "step 6902\t classif : 0.95520"
     ]
    }
   ],
   "source": [
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "wave_tensor = wave_tensor_data(x_test)\n",
    "classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                        x_4: wave_tensor[4],\\\n",
    "                                        x_3: wave_tensor[3],\\\n",
    "                                        x_2: wave_tensor[2],\\\n",
    "                                        x_1: wave_tensor[1],\\\n",
    "                                        x_0: wave_tensor[0],\\\n",
    "                                        y: y_test,\\\n",
    "                                        keep_prob: 1,\\\n",
    "                                        batch_phase:False})\n",
    "sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                 % (num_epoch, \\\n",
    "                    classif_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "step 5729\t classif : 0.96990"
     ]
    }
   ],
   "source": [
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "wave_tensor = wave_tensor_data(x_test)\n",
    "classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                        x_4: wave_tensor[4],\\\n",
    "                                        x_3: wave_tensor[3],\\\n",
    "                                        x_2: wave_tensor[2],\\\n",
    "                                        x_1: wave_tensor[1],\\\n",
    "                                        x_0: wave_tensor[0],\\\n",
    "                                        y: y_test,\\\n",
    "                                        keep_prob: 1,\\\n",
    "                                        batch_phase:False})\n",
    "sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                 % (num_epoch, \\\n",
    "                    classif_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "step 5339\t classif : 0.97120"
     ]
    }
   ],
   "source": [
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "wave_tensor = wave_tensor_data(x_test)\n",
    "classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                        x_4: wave_tensor[4],\\\n",
    "                                        x_3: wave_tensor[3],\\\n",
    "                                        x_2: wave_tensor[2],\\\n",
    "                                        x_1: wave_tensor[1],\\\n",
    "                                        x_0: wave_tensor[0],\\\n",
    "                                        y: y_test,\\\n",
    "                                        keep_prob: 1,\\\n",
    "                                        batch_phase:False})\n",
    "sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                 % (num_epoch, \\\n",
    "                    classif_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "step 17525\t classif : 0.97750"
     ]
    }
   ],
   "source": [
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "wave_tensor = wave_tensor_data(x_test)\n",
    "classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                        x_4: wave_tensor[4],\\\n",
    "                                        x_3: wave_tensor[3],\\\n",
    "                                        x_2: wave_tensor[2],\\\n",
    "                                        x_1: wave_tensor[1],\\\n",
    "                                        x_0: wave_tensor[0],\\\n",
    "                                        y: y_test,\\\n",
    "                                        keep_prob: 1,\\\n",
    "                                        batch_phase:False})\n",
    "sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                 % (num_epoch, \\\n",
    "                    classif_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "step 27387\t classif : 0.97760"
     ]
    }
   ],
   "source": [
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "wave_tensor = wave_tensor_data(x_test)\n",
    "classif_eval = accuracy.eval(feed_dict={x_5: wave_tensor[5],\\\n",
    "                                        x_4: wave_tensor[4],\\\n",
    "                                        x_3: wave_tensor[3],\\\n",
    "                                        x_2: wave_tensor[2],\\\n",
    "                                        x_1: wave_tensor[1],\\\n",
    "                                        x_0: wave_tensor[0],\\\n",
    "                                        y: y_test,\\\n",
    "                                        keep_prob: 1,\\\n",
    "                                        batch_phase:False})\n",
    "sys.stdout.write('\\rstep %d\\t classif : %.5f' \\\n",
    "                 % (num_epoch, \\\n",
    "                    classif_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
