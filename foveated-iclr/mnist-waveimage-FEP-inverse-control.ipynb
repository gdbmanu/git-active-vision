{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : generaliser Ã  images 32x32, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32\n",
    "from waveimage import WaveImage, calc_dim, calc_U, mnist_reshape_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waveimage import calc_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "axes = []\n",
    "h_max = 6\n",
    "shape = (32,32)\n",
    "\n",
    "U = {}\n",
    "for h in range(h_max):\n",
    "    U [h] = {}\n",
    "    dim_i, dim_j = calc_dim(shape, h, h_max)\n",
    "    for i in range(dim_i):\n",
    "        for j in range(dim_j):\n",
    "            U[h][(i,j)] = 1    \n",
    "\n",
    "NB_POS = len(U[h_max - 1])\n",
    "print NB_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creation de la base d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_tensor_data(batch_x):\n",
    "    batch_size, _ = batch_x.shape\n",
    "    wave_tensor = {}\n",
    "    for h in range(6):\n",
    "        if h == 0:\n",
    "            h_size = 1\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 1))\n",
    "        else:\n",
    "            h_size = 2**(h - 1)\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 3))\n",
    "    for num_batch in range(batch_size):\n",
    "        image = mnist_reshape_32(batch_x[num_batch])\n",
    "        w = WaveImage(image = image)\n",
    "        for h in range(w.get_h_max()):\n",
    "            data_h = w.get_data()[h]\n",
    "            if h == 0:\n",
    "                wave_tensor[h][num_batch][0][0][0] = data_h[(0,0)]\n",
    "            else:\n",
    "                for u in data_h:\n",
    "                    wave_tensor[h][num_batch][u[0]][u[1]][:] = data_h[u]\n",
    "    return wave_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pow2(i_ref):\n",
    "    pow2_i = np.zeros(5, dtype='int')\n",
    "    reste = i_ref\n",
    "    for p in range(4,-1,-1):\n",
    "        pow2_i[p] = int(reste // 2**p)\n",
    "        #reste = reste % 2**p\n",
    "    return pow2_i[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  7 15]\n"
     ]
    }
   ],
   "source": [
    "print calc_pow2(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wave_tensor(batch_size):\n",
    "    wave_tensor = {}\n",
    "    for h in range(6):\n",
    "        if h == 0:\n",
    "            h_size = 1\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 1))\n",
    "        else:\n",
    "            h_size = 2**(h - 1)\n",
    "            wave_tensor[h] = np.zeros((batch_size, h_size, h_size, 3))\n",
    "    return wave_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_tensor_data_backbone(batch_x, depth = -1, i_ref = -1, j_ref = -1):\n",
    "    batch_size, _ = batch_x.shape\n",
    "    FLAG_RAND_I = i_ref == -1\n",
    "    FLAG_RAND_J = j_ref == -1\n",
    "    FLAG_DEPTH = depth == -1\n",
    "    wave_tensor = init_wave_tensor(batch_size)\n",
    "    for num_batch in range(batch_size):\n",
    "        image = mnist_reshape_32(batch_x[num_batch])\n",
    "        w = WaveImage(image = image)\n",
    "        if FLAG_RAND_I:\n",
    "            i_ref = np.random.randint(16)\n",
    "        if FLAG_RAND_J:\n",
    "            j_ref = np.random.randint(16)  \n",
    "        if FLAG_DEPTH:\n",
    "            depth = 1 + np.random.randint(6)\n",
    "        pow2_i = calc_pow2(i_ref)\n",
    "        pow2_j = calc_pow2(j_ref)\n",
    "        for h in range(6 - depth, 6):\n",
    "            data_h = w.get_data()[h]\n",
    "            if h == 0:\n",
    "                wave_tensor[h][num_batch][0][0][0] = data_h[(0,0)] #/ 4**4\n",
    "            else:\n",
    "                u = (pow2_i[h - 1], pow2_j[h - 1])\n",
    "                #for u in data_h:\n",
    "                #    wave_tensor[h][num_batch][u[0]][u[1]][:] = 0\n",
    "                wave_tensor[h][num_batch][u[0]][u[1]][:] = data_h[u] #/ 4 ** (5 - h)\n",
    "    return wave_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction \n",
    "+ 5 couches convolutionnelles : 16 x 16 --> 8 x 8 ; 8 x 8 --> 4 x 4 etc\n",
    "+ 1 couche FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_WAV = 3\n",
    "NB_LABEL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood calculation (over u's, for given v, h and c)\n",
    "\n",
    "def calc_lik_u(v,h,c):\n",
    "    h_ref = 5\n",
    "    '''if h > 0:\n",
    "        nb_lik = NB_POS / 4 ** (h_ref - h)\n",
    "        div = 2 ** (h - 1)\n",
    "    else:\n",
    "        nb_lik = 1\n",
    "        div = 1'''\n",
    "    lik = np.zeros(NB_POS)\n",
    "    for ind_u in range(NB_POS):\n",
    "        u_ref = (ind_u / 16, ind_u % 16)\n",
    "        if h > 0:\n",
    "            u_path = (u_ref[0] / 2 ** (h_ref - h), u_ref[1] / 2 ** (h_ref - h))\n",
    "        else:\n",
    "            u_path = (0, 0)\n",
    "        if np.linalg.norm(v) < 1e-16:\n",
    "            if np.linalg.norm(mu[c][h][u_path]) > 1e-16:\n",
    "                lik[ind_u] = rho[c][h][u_path]\n",
    "            else:\n",
    "                lik[ind_u] = 1                \n",
    "        else:\n",
    "            if np.linalg.norm(mu[c][h][u_path]) > 1e-16:   \n",
    "                if h == 0:\n",
    "                    dist = multivariate_normal(mean = mu[c][h][u_path], cov = Sigma[c][h][u_path])\n",
    "                else:\n",
    "                    dist = multivariate_normal(mean = mu[c][h][u_path], cov = Sigma[c][h][u_path] + 1e-10 * np.eye(3))\n",
    "                lik[ind_u] = (1-rho[c][h][u_path]) * dist.pdf(v)\n",
    "                #lik[c] =  dist.pdf(v)\n",
    "            else:\n",
    "                lik[ind_u] = 0\n",
    "        lik[ind_u] = max(lik[ind_u],1e-16)    \n",
    "        #print c, h, u_ref, lik\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log posterior\n",
    "\n",
    "def update_log_score(log_score, lik):\n",
    "    #print 'lik =' + str(lik) \n",
    "    log_score += np.log(lik) \n",
    "    max_log_score = max(log_score)\n",
    "    log_score -= max_log_score\n",
    "    return log_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior (Softmax)\n",
    "    \n",
    "def calc_pi(log_score): # TODO\n",
    "    Z = np.sum(np.exp(log_score))\n",
    "    pi = np.exp(log_score)/Z\n",
    "    #print 'pi =' + str(pi)\n",
    "    #print 'max(pi) = ',max(pi)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcours predictif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, Sigma, rho = pickle.load(open(\"mnist-waveimage-train-mu-Sigma-rho.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_predictive_eff = pickle.load(open(\"mnist-waveimage-saliency-map.pkl\", \"rb\"))    \n",
    "#pi_predictive_eff = pickle.load(open(\"mnist-waveimage-saliency-map-diff-backbone-CNN-parts.pkl\", \"rb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_generic_eff = pickle.load(open(\"mnist-waveimage-generic-saliency-map.pkl\", \"rb\"))    \n",
    "#pi_predictive_eff = pickle.load(open(\"mnist-waveimage-saliency-map-diff-backbone-CNN-parts.pkl\", \"rb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_asc_path(h,u):\n",
    "    rep = []\n",
    "    for h_inf in range(h, 0, -1):\n",
    "        i_inf = u[0] / (2 ** (h - h_inf))\n",
    "        j_inf = u[1] / (2 ** (h - h_inf))\n",
    "        rep += [(h_inf, (i_inf, j_inf))]\n",
    "    # racine\n",
    "    rep += [(0, (i_inf, j_inf))]\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi_predictive_sorted(pi_predictive): \n",
    "    pi_predictive_sorted = {}\n",
    "    for c in range(10):\n",
    "        pi_predictive_sorted[c] = []\n",
    "        for k in pi_predictive[c]:\n",
    "            pi_predictive_sorted[c] += [(pi_predictive[c][k], k)]\n",
    "        pi_predictive_sorted[c] = sorted(pi_predictive_sorted[c])\n",
    "    return pi_predictive_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_H_predictive_sorted(H_predictive): \n",
    "    H_predictive_sorted = []\n",
    "    for k in H_predictive:\n",
    "        H_predictive_sorted += [(H_predictive[k], k)]\n",
    "    H_predictive_sorted = sorted(H_predictive_sorted, reverse=True)\n",
    "    return H_predictive_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_generator(c, h, u):\n",
    "    test_pred = rho[c][h][u] < .5       \n",
    "    if test_pred:\n",
    "        return mu[c][h][u]\n",
    "    else:\n",
    "        return np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_generator(log_score, h, u):\n",
    "    Z = np.sum(np.exp(log_score))\n",
    "    mu_c = np.zeros(3)\n",
    "    for c in range(NB_LABEL):\n",
    "        pi = np.exp(log_score[0][c]) / Z\n",
    "        mu_c += pi * mu[c][h][u] * (1 - rho[c][h][u])\n",
    "    return mu_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "h_max = 6\n",
    "lik_predictive = {}\n",
    "for c in range(10):\n",
    "    print c\n",
    "    lik_predictive[c] = {}\n",
    "    for h in range(h_max):\n",
    "        lik_predictive[c][h] = {}\n",
    "        for u in U[h]:\n",
    "            v_predictive = argmax_generator(c, h, u)\n",
    "            lik = calc_lik_u(v_predictive, h, c)\n",
    "            lik_predictive[c][h][u] = lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_search_u(u_tilde, log_score, z_actions_set, FLAG_DUAL = False, FLAG_KL = False):\n",
    "    # actions_set ne contient que les positions de niveau 5\n",
    "    h_ref = 5\n",
    "    batch_size = len(z_actions_set)\n",
    "    #print batch_size\n",
    "    ## Parcours predictif\n",
    "    log_score_path = np.zeros((batch_size, NB_POS))\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        log_score_path[i] = np.copy(log_score[0])\n",
    "    dict_z = {}\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        dict_z[i] = z_gen\n",
    "        liste_path = calcule_asc_path(h_ref, u_tilde)\n",
    "        #print(liste_path)\n",
    "        for (h_path, u_path) in liste_path[:-1]:\n",
    "            #print(h_path, u_path)\n",
    "            log_score_path[i] = update_log_score(log_score_path[i], lik_predictive[z_gen][h_path][u_path])\n",
    "        #print log_score_path[i]\n",
    "        \n",
    "    q_pre = np.exp(log_score[0]) / np.sum(np.exp(log_score[0]))\n",
    "    FEP_post = np.zeros(batch_size)\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        q_post = np.exp(log_score_path[i]) / np.sum(np.exp(log_score_path[i]))  \n",
    "        if not FLAG_DUAL:\n",
    "            FEP_post[i] = entropy(q_post)\n",
    "        else:\n",
    "            delta_log_score_path = log_score_path[i] - log_score[0]\n",
    "            delta_q_post = np.exp(delta_log_score_path) / np.sum(np.exp(delta_log_score_path))\n",
    "            if not FLAG_KL:\n",
    "                FEP_post[i] = entropy(q_post) - np.log(delta_q_post[u_tilde]) #+ np.log(q_pre[z_ref]) \n",
    "            else:\n",
    "                FEP_post[i] = - np.log(delta_q_post[u_ref])\n",
    "            #FEP_post[i] = - np.log(delta_q_post[z_ref]) #+ np.log(q_pre[c])\n",
    "            #print u_gen, entropy(q_post), - np.log(delta_q_post[z_ref]), FEP_post[i] \n",
    "                \n",
    "    #i_max = np.where(log_score_path[:, z_ref] == max(log_score_path[:, z_ref]))[0][0]\n",
    "    i_min = np.where(FEP_post == min(FEP_post))[0][0]\n",
    "    q_post = np.exp(log_score_path[i_min]) / np.sum(np.exp(log_score_path[i_min])) \n",
    "    delta_log_score_path = log_score_path[i_min] - log_score[0]\n",
    "    delta_q_post = np.exp(delta_log_score_path) / np.sum(np.exp(delta_log_score_path))\n",
    "    #print log_score_path[i_min] \n",
    "    #print dict_u[i_min] , entropy(q_post), - np.log(delta_q_post[z_ref]) + np.log(q_pre[z_ref]), FEP_post[i_min]\n",
    "    \n",
    "    \n",
    "    ## 3 ##\n",
    "    return dict_z[i_min] #, pi_path[i_max] #pi_path[i_max][z_ref]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEP_predictive_search_full(log_score, z_actions_set, FLAG_DUAL = False, FLAG_KL = False):\n",
    "    # actions_set ne contient que les positions de niveau 5\n",
    "    h_ref = 5\n",
    "    batch_ref = len(z_actions_set)\n",
    "    batch_size = batch_ref * NB_POS\n",
    "    log_score_path = {}\n",
    "    \n",
    "    ## Parcours predictif\n",
    "    log_score_path = np.zeros((batch_size, NB_POS))\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        for ind_u in range(NB_POS):\n",
    "            i_full = ind_u * batch_ref + i\n",
    "            log_score_path[i_full] = np.copy(log_score[0])\n",
    "    dict_z = {}\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        dict_z[i] = z_gen\n",
    "        for ind_u, u_gen in enumerate(U[h_ref]):\n",
    "            liste_path = calcule_asc_path(h_ref, u_gen)\n",
    "            #print(liste_path)\n",
    "            for (h_path, u_path) in liste_path[:-1]:\n",
    "                #print(h_path, u_path)\n",
    "                    i_full = ind_u * batch_ref + i\n",
    "                    log_score_path[i_full] = update_log_score(log_score_path[i_full],\\\n",
    "                                                              lik_predictive[z_gen][h_path][u_path])\n",
    "                        \n",
    "    FEP_post = np.zeros(batch_size)\n",
    "    q_pre = np.exp(log_score[0]) / np.sum(np.exp(log_score[0]))\n",
    "    #print q_pre\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        for ind_u, u_gen in enumerate(U[h_ref]):\n",
    "            i_full = ind_u * batch_ref + i\n",
    "            q_post = np.exp(log_score_path[i_full]) / np.sum(np.exp(log_score_path[i_full]))  \n",
    "            if not FLAG_DUAL:\n",
    "                FEP_post[i_full] = entropy(q_post)\n",
    "                #print u_gen, q_post\n",
    "                #print FEP_post[i]\n",
    "            else:\n",
    "                delta_log_score_path = log_score_path[i_full] - log_score[0]\n",
    "                delta_q_post = np.exp(delta_log_score_path) / np.sum(np.exp(delta_log_score_path))\n",
    "                #q_pre_hat = np.zeros(NB_LABEL)\n",
    "                #q_pre_hat[c] = 1\n",
    "                if not FLAG_KL:\n",
    "                    FEP_post[i_full] = entropy(q_post) - np.log(delta_q_post[z_gen]) #+ np.log(q_pre[c])\n",
    "                else:\n",
    "                    FEP_post[i_full] = - np.log(delta_q_post[z_gen]) \n",
    "                #FEP_post[i_full] = - np.log(delta_q_post[c]) #+ np.log(q_pre[c])\n",
    "                #FEP_post[i_full] = entropy(q_post) + entropy(q_pre, delta_q_post)\n",
    "                #print u_gen, entropy(q_post), - np.log(delta_q_post[c]), FEP_post[i_full] \n",
    "    #ch = raw_input('')        \n",
    "    FEP_post_full = np.zeros((batch_ref,))\n",
    "    for ind_u in range(NB_POS):\n",
    "        FEP_post_full += q_pre[ind_u] * FEP_post[ind_u * batch_ref : (ind_u + 1) * batch_ref]\n",
    "        \n",
    "    #for i, u_gen in enumerate(actions_set):\n",
    "    #    print u_gen, FEP_post_full[i]\n",
    "    \n",
    "    i_min = np.where(FEP_post_full == min(FEP_post_full))[0][0]\n",
    "    \n",
    "    ## 3 ##\n",
    "    return dict_z[i_min]  #pi_path[i_max][z_ref]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_based_policy(log_score, z_actions_set, FLAG_DUAL = False, FLAG_KL = False):\n",
    "    \n",
    "    u_tilde = np.argmax(log_score)    \n",
    "    z_tilde = predictive_search(u_tilde, log_score, z_actions_set, FLAG_DUAL, FLAG_KL)\n",
    "    \n",
    "    return z_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEP_prediction_based_policy(log_score, z_actions_set, FLAG_DUAL = False, FLAG_KL = False):\n",
    "    \n",
    "    z_tilde = FEP_predictive_search_full(log_score, z_actions_set, FLAG_DUAL, FLAG_KL)\n",
    "    \n",
    "    return z_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(log_score, z_actions_set):\n",
    "    h = 5\n",
    "    ## 1 ##\n",
    "    u_tilde = np.argmax(log_score)\n",
    "    #u_tilde = (1 + np.random.randint(14),  1 + np.random.randint(14))\n",
    "    dict_z = {}\n",
    "    for i, z_gen in enumerate(z_actions_set):\n",
    "        dict_z[i] = z_gen\n",
    "    z_tilde = dict_z[np.random.randint(len(dict_z))]\n",
    "    return z_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_exploration(wave_tensor_ref, log_score, u_ref, ind_test, z_actions_set, record, \\\n",
    "                      POL = 'FEP-predictive', AFF = True, THRESHOLD = 1e-4):\n",
    "    \n",
    "    assert  POL == 'random' or POL == 'full'\\\n",
    "    or POL == 'FEP-predictive' or POL == 'FEP-predictive-dual'\n",
    "    \n",
    "    if POL == 'full':\n",
    "        THRESHOLD = 0\n",
    "        POL = 'saliency-based'\n",
    "    \n",
    "    TOUR = 0\n",
    "    END = False\n",
    "    h_ref = 5\n",
    "    \n",
    "    # saliency-based approach\n",
    "\n",
    "    while END == False:\n",
    "        if AFF:\n",
    "            print '************************************'\n",
    "            print '******       TOUR    ' + str(TOUR + 1) + '        ******'\n",
    "            print '************************************'\n",
    "        \n",
    "        # 1. CHOIX\n",
    "        if POL == 'FEP-predictive-dual':\n",
    "            z_tilde = TOUR #FEP_prediction_based_policy(log_score, z_actions_set,\\\n",
    "                           #                           FLAG_DUAL = True)\n",
    "            #ch = raw_input('')     \n",
    "        elif POL == 'FEP-predictive':\n",
    "            z_tilde = TOUR #FEP_prediction_based_policy(log_score, z_actions_set,\\\n",
    "                           #                           FLAG_DUAL = False)\n",
    "        else:\n",
    "            z_tilde = random_policy(log_score, z_actions_set)\n",
    "        \n",
    "        if AFF:\n",
    "            print 'CHOIX :', z_tilde\n",
    "        \n",
    "        # 2. LECTURE + UPDATE\n",
    "        #wave_tensor =  init_wave_tensor(1)\n",
    "        liste_path = calcule_asc_path(h_ref, u_ref)\n",
    "        \n",
    "        for (h_path, u_path) in reversed(liste_path):\n",
    "            v = wave_tensor_ref[h_path][z_tilde][u_path[0]][u_path[1]][:]\n",
    "            lik = calc_lik_u(v, h_path, z_tilde)\n",
    "            log_score[0] = update_log_score(log_score[0], lik)\n",
    "            #wave_tensor[h_path][0][u_path[0]][u_path[1]][:] =  \n",
    "            record.nb_coeffs += 3  \n",
    "                \n",
    "        pi = np.exp(log_score[0]) / np.sum(np.exp(log_score[0])) #sess.run(tf.nn.softmax(log_score))[0]\n",
    "                \n",
    "        H = entropy(pi) # sess.run(tf.nn.softmax_cross_entropy_with_logits(labels=pi,logits=log_score)) #np.sum(- pi * np.log(pi))\n",
    "        out = np.argmax(pi)\n",
    "        out_i = out / 16\n",
    "        out_j = out % 16\n",
    "                \n",
    "        if AFF :\n",
    "            #print 'pi : ', pi\n",
    "            print 'out :', out,  (out_i, out_j)\n",
    "            print 'pi[out] : ', pi[out]\n",
    "            print 'H : ', H\n",
    "\n",
    "        record.mem_pi += [pi]\n",
    "        record.mem_H += [H]\n",
    "        record.mem_u += [(out_i, out_j)]\n",
    "                \n",
    "        # 3. INHIBITION OF RETURN        \n",
    "        z_actions_set.pop(z_tilde)\n",
    "        \n",
    "        record.mem_z += [z_tilde]\n",
    "        record.nb_saccades += 1\n",
    "                \n",
    "        if AFF:\n",
    "            print '****', 'u :', u_ref, ', z :',z_tilde, ' ---> ', (out_i, out_j)\n",
    "              \n",
    "        \n",
    "        if TOUR == NB_LABEL - 1  :#or H < THRESHOLD:\n",
    "            END = True\n",
    "            if AFF :\n",
    "                print '************************************'\n",
    "                print '******         FINI          *******'\n",
    "                print '************************************' \n",
    "            return (out_i, out_j)\n",
    "        else:\n",
    "            TOUR += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from record import Record, affiche_records            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRIALS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generic saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test = [[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(mnist.test.images)):\n",
    "    c = mnist.train.labels[i]\n",
    "    Data_test[c] += [mnist.train.images[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FEP-predictive-dual', 0.1, 0)\n",
      "************************************\n",
      "******       TOUR    1        ******\n",
      "************************************\n",
      "CHOIX : 0\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0315590932548\n",
      "H :  3.75744977495\n",
      "**** u : (2, 2) , z : 0  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    2        ******\n",
      "************************************\n",
      "CHOIX : 1\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.033559763427\n",
      "H :  3.69441684745\n",
      "**** u : (2, 2) , z : 1  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    3        ******\n",
      "************************************\n",
      "CHOIX : 2\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0396339377554\n",
      "H :  3.49393521022\n",
      "**** u : (2, 2) , z : 2  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    4        ******\n",
      "************************************\n",
      "CHOIX : 3\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0443880637178\n",
      "H :  3.39793372708\n",
      "**** u : (2, 2) , z : 3  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    5        ******\n",
      "************************************\n",
      "CHOIX : 4\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0477753836731\n",
      "H :  3.34324873483\n",
      "**** u : (2, 2) , z : 4  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    6        ******\n",
      "************************************\n",
      "CHOIX : 5\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0489652145761\n",
      "H :  3.31823828929\n",
      "**** u : (2, 2) , z : 5  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    7        ******\n",
      "************************************\n",
      "CHOIX : 6\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0496689057381\n",
      "H :  3.3025794402\n",
      "**** u : (2, 2) , z : 6  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    8        ******\n",
      "************************************\n",
      "CHOIX : 7\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0562659885154\n",
      "H :  3.1492954243\n",
      "**** u : (2, 2) , z : 7  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    9        ******\n",
      "************************************\n",
      "CHOIX : 8\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0569673949169\n",
      "H :  3.12792570773\n",
      "**** u : (2, 2) , z : 8  --->  (0, 0)\n",
      "************************************\n",
      "******       TOUR    10        ******\n",
      "************************************\n",
      "CHOIX : 9\n",
      "out : 0 (0, 0)\n",
      "pi[out] :  0.0582021357592\n",
      "H :  3.09101910748\n",
      "**** u : (2, 2) , z : 9  --->  (0, 0)\n",
      "************************************\n",
      "******         FINI          *******\n",
      "************************************\n",
      "Policy : FEP-predictive-dual, threshold : 0.1, 10 saccades, final : (0, 0), classe : (2, 2), elapsed time : 2.29541\n",
      "\n",
      "\n",
      "Nb trials : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "dict_records = {}\n",
    "\n",
    "#file_name = \"mnist-waveimage-CNN-backbone-records-rnd-parts-generic-saliency.npy\"\n",
    "file_name = \"mnist-waveimage-records-FEP-inverse-control.npy\"\n",
    "\n",
    "if True: # not os.path.isfile(file_name):\n",
    "    \n",
    "    for POL in ('FEP-predictive-dual',): # 'FEP-predictive', 'random'): # 'FEP-predictive'): #'generic-saliency-based',): \n",
    "\n",
    "        dict_records[POL] = {}\n",
    "\n",
    "        for THRESHOLD in (1e-1,):#, 1e-2, 1e-3):\n",
    "\n",
    "            records = [] \n",
    "            cpt_TRIALS = 0\n",
    "\n",
    "            tic = time.time()\n",
    "\n",
    "            #NB_TRIALS = len(Data_test[z_ref][0][(0,0)])\n",
    "\n",
    "            for ind_test in range(NB_TRIALS):\n",
    "                if ind_test % 10 == 0:\n",
    "                    print(POL, THRESHOLD, ind_test)\n",
    "\n",
    "                batch_x = np.zeros((10, 28 * 28))\n",
    "                for c in range(10):\n",
    "                    x_test = Data_test[c][ind_test]\n",
    "                    batch_x[c,:] = x_test\n",
    "                wave_tensor_ref = wave_tensor_data(batch_x)\n",
    "                u_ref = (np.random.randint(16), np.random.randint(16))\n",
    "                # + 0.1 * np.random.randn(28 * 28)\n",
    "                # \n",
    "                # initial\n",
    "                log_score = np.zeros((1,NB_POS))\n",
    "                pi = np.ones(NB_POS) / NB_POS\n",
    "                H = entropy(pi) #np.sum(- pi * np.log(pi))\n",
    "\n",
    "                record = Record()\n",
    "                record.POL = POL\n",
    "                record.THRESHOLD = THRESHOLD\n",
    "                record.u_ref = u_ref\n",
    "                record.mem_pi += [pi]\n",
    "                record.mem_H += [H]\n",
    "                u_tilde = -1\n",
    "\n",
    "                # global coef --> log_score initial\n",
    "                '''mem_h_u = [(0, (0, 0)), (1, (0, 0))]\n",
    "                h, u = 0, (0, 0)\n",
    "                v = wave_tensor_ref[h][0][u[0]][u[1]][:]\n",
    "                lik = calc_lik(v,h,u)\n",
    "                log_score = update_log_score(log_score, lik)\n",
    "                h, u = 1, (0, 0)\n",
    "                v = wave_tensor_ref[h][0][u[0]][u[1]][:]\n",
    "                lik = calc_lik(v,h,u)\n",
    "                log_score = update_log_score(log_score, lik)\n",
    "                pi = calc_pi(log_score)\n",
    "                H = entropy(pi) #np.sum(- pi * np.log(pi))\n",
    "                z_tilde = np.argmax(log_score)\n",
    "\n",
    "                record.mem_pi += [pi]\n",
    "                record.mem_H += [H]\n",
    "                record.mem_z += [z_tilde]\n",
    "                record.nb_coeffs += 1'''\n",
    "\n",
    "                # initial actions set\n",
    "                z_actions_set = {}\n",
    "                for c in range(NB_LABEL):\n",
    "                    z_actions_set[c] = 1\n",
    "\n",
    "                u_final = scene_exploration(wave_tensor_ref, log_score, u_ref, ind_test,\\\n",
    "                                            z_actions_set, record, \\\n",
    "                                            POL = POL, AFF = True, THRESHOLD = THRESHOLD)\n",
    "                record.u_final = u_final\n",
    "                record.success = u_ref == u_final\n",
    "\n",
    "                records += [record]\n",
    "\n",
    "                toc = time.time()\n",
    "                if NB_TRIALS <= 1000:\n",
    "                    print '\\rPolicy : %s, threshold : %g, %d saccades, final : (%d, %d), classe : (%d, %d), elapsed time : %g' \\\n",
    "                                    % (POL, THRESHOLD, record.nb_saccades, u_final[0], u_final[1], u_ref[0], u_ref[1], toc - tic)   \n",
    "                cpt_TRIALS  += NB_TRIALS\n",
    "\n",
    "            dict_records[POL][THRESHOLD] = records\n",
    "            print '\\n'\n",
    "            print 'Nb trials :', cpt_TRIALS\n",
    "            #affiche_records(records)\n",
    "            print '\\n'\n",
    "            np.save(file_name, dict_records)\n",
    "else:\n",
    "    dict_records = np.load(file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy : FEP-predictive, threshold : 0.1, 2 saccades, initial : -1, final : 7, classe : 7, elapsed time : 0.290342\n",
    "Policy : FEP-predictive, threshold : 0.1, 1 saccades, initial : -1, final : 6, classe : 2, elapsed time : 0.443286\n",
    "Policy : FEP-predictive, threshold : 0.1, 1 saccades, initial : -1, final : 1, classe : 1, elapsed time : 0.587564\n",
    "Policy : FEP-predictive, threshold : 0.1, 1 saccades, initial : -1, final : 0, classe : 0, elapsed time : 0.736176\n",
    "Policy : FEP-predictive, threshold : 0.1, 2 saccades, initial : -1, final : 4, classe : 4, elapsed time : 0.991568\n",
    "Policy : FEP-predictive, threshold : 0.1, 1 saccades, initial : -1, final : 1, classe : 1, elapsed time : 1.14426\n",
    "Policy : FEP-predictive, threshold : 0.1, 2 saccades, initial : -1, final : 4, classe : 4, elapsed time : 1.446\n",
    "Policy : FEP-predictive, threshold : 0.1, 2 saccades, initial : -1, final : 9, classe : 9, elapsed time : 1.69376\n",
    "Policy : FEP-predictive, threshold : 0.1, 2 saccades, initial : -1, final : 6, classe : 5, elapsed time : 1.95663\n",
    "Policy : FEP-predictive, threshold : 0.1, 3 saccades, initial : -1, final : 9, classe : 9, elapsed time : 2.32355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
