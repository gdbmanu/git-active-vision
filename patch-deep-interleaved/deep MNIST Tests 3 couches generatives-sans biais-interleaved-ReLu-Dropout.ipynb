{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couche de poids paramétrable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev = 0.1, name = \"dummy\"):\n",
    "    #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    initial = tf.random_normal(shape, stddev)\n",
    "    #initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convertisseur image 32 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_reshape_32(x_batch):\n",
    "    if len(x_batch.shape) > 1 :\n",
    "        m, _ = x_batch.shape\n",
    "        batch32 = np.zeros((m, 1024))\n",
    "        for i, x in enumerate(x_batch):\n",
    "            assert x.shape == (28 * 28,)\n",
    "            image = x.reshape(28, 28)\n",
    "            image = np.append(np.zeros((2, 28)), image, axis = 0)\n",
    "            image = np.append(image, np.zeros((2, 28)), axis = 0)\n",
    "            image = np.append(np.zeros((32, 2)), image, axis = 1)\n",
    "            image = np.append(image, np.zeros((32, 2)), axis = 1)\n",
    "            batch32[i,:] = image.reshape((32 * 32,))\n",
    "    else:\n",
    "        m = x_batch.shape\n",
    "        batch32 = np.zeros((1024,))\n",
    "        assert x_batch.shape == (28 * 28,)\n",
    "        image = x_batch.reshape(28, 28)\n",
    "        image = np.append(np.zeros((2, 28)), image, axis = 0)\n",
    "        image = np.append(image, np.zeros((2, 28)), axis = 0)\n",
    "        image = np.append(np.zeros((32, 2)), image, axis = 1)\n",
    "        image = np.append(image, np.zeros((32, 2)), axis = 1)\n",
    "        batch32 = image.reshape((1,32 * 32))\n",
    "    return batch32 #.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction couche 1 : 32 x 32 --> 16 x 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH1 = 8\n",
    "WIDTH1 = 4\n",
    "OUT1 = 32 / WIDTH1\n",
    "\n",
    "DEPTH2 = 32\n",
    "WIDTH2 = 4\n",
    "OUT2 = 32 / WIDTH1 / WIDTH2\n",
    "\n",
    "DEPTH3 = 64 #128\n",
    "WIDTH3 = 2\n",
    "OUT3 = 32 / WIDTH1 / WIDTH2 / WIDTH3\n",
    "\n",
    "\n",
    "#DEPTH4 = 512\n",
    "\n",
    "#DEPTH1 = 8\n",
    "#DEPTH2 = 16\n",
    "#DEPTH3 = 32\n",
    "#DEPTH4 = 64\n",
    "\n",
    "STD = 1.\n",
    "\n",
    "BATCH_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data placeholders\n",
    "\n",
    "(les données sont empilées selon dimension 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data placeholders\n",
    "\n",
    "(les données sont empilées selon dimension 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 1024])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "training_step = tf.placeholder(tf.float32, shape=[4])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "W_conv1 = weight_variable([WIDTH1, WIDTH1, 1, DEPTH1], stddev = STD / (WIDTH1 * WIDTH1), name = \"W_conv1\")\n",
    "# Input 2D reshape\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 1])\n",
    "# Graph construction\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, WIDTH1, WIDTH1, 1], padding='VALID') \n",
    "#z_conv1 = tf.nn.relu(h_conv1)\n",
    "z_conv1 = tf.nn.relu(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([WIDTH2, WIDTH2, DEPTH1, DEPTH2], stddev = STD / (WIDTH2 * WIDTH2 * DEPTH1), name = \"W_conv2\")\n",
    "h_conv2 = tf.nn.conv2d(z_conv1, W_conv2, strides=[1, WIDTH2, WIDTH2, 1], padding='VALID') \n",
    "z_conv2 = tf.nn.relu(h_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([WIDTH3, WIDTH3, DEPTH2, DEPTH3], stddev = STD / (WIDTH3 * WIDTH3 * DEPTH2), name = \"W_conv3\")\n",
    "h_conv3 = tf.nn.conv2d(z_conv2, W_conv3, strides=[1, WIDTH3, WIDTH3, 1], padding='VALID') \n",
    "z_conv3 = tf.nn.relu(h_conv3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative graph\n",
    "  - incremental learning\n",
    "  - ladder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_flat1 = tf.nn.dropout(tf.reshape(z_conv1, [-1, OUT1 * OUT1 * DEPTH1]), keep_prob)\n",
    "z_flat2 = tf.nn.dropout(tf.reshape(z_conv2, [-1, OUT2 * OUT2 * DEPTH2]), keep_prob)\n",
    "z_flat3 = tf.nn.dropout(tf.reshape(z_conv3, [-1, OUT3 * OUT3 * DEPTH3]), keep_prob)\n",
    "\n",
    "W_out1 = weight_variable([OUT1 * OUT1 * DEPTH1, 10], stddev = STD / (OUT1 * OUT1 * DEPTH1), name = \"W_out1\")\n",
    "W_out2 = weight_variable([OUT2 * OUT2 * DEPTH2, 10], stddev = STD / (OUT2 * OUT2 * DEPTH2), name = \"W_out2\")\n",
    "W_out3 = weight_variable([OUT3 * OUT3 * DEPTH3, 10], stddev = STD / (OUT3 * OUT3 * DEPTH3), name = \"W_out3\")\n",
    "\n",
    "y_cnn =   tf.scalar_mul(training_step[0], tf.matmul(z_flat1, W_out1)) \\\n",
    "        + tf.scalar_mul(training_step[1], tf.matmul(z_flat2, W_out2)) \\\n",
    "        + tf.scalar_mul(1- (1-training_step[2]) * (1-training_step[3]), tf.matmul(z_flat3, W_out3)) \n",
    "\n",
    "z_cnn = tf.nn.softmax(y_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_deconv3 = weight_variable([10, OUT3 * OUT3 * DEPTH3], stddev = STD / 10, name = \"W_deconv3\")\n",
    "z_in3 = tf.scalar_mul(training_step[3], z_cnn)\n",
    "h_deconv3_flat = tf.matmul(z_in3, W_deconv3)\n",
    "h_deconv3 = tf.reshape(h_deconv3_flat, [-1, OUT3, OUT3,  DEPTH3])\n",
    "z_deconv3 = tf.nn.softmax(h_deconv3)\n",
    "\n",
    "W_deconv2 = weight_variable([WIDTH3, WIDTH3, DEPTH2, DEPTH3], stddev = STD / (WIDTH3 * WIDTH3 * DEPTH2), name = \"W_deconv2\")\n",
    "z_in2 = tf.scalar_mul(training_step[2], tf.nn.softmax(h_conv3)) \\\n",
    "        + tf.scalar_mul(training_step[3], z_deconv3)\n",
    "\n",
    "h_deconv2 = tf.nn.conv2d_transpose(z_in2, W_deconv2, output_shape=[BATCH_SIZE, OUT2, OUT2, DEPTH2],\n",
    "                                        strides=[1, WIDTH3, WIDTH3, 1], padding='VALID') \n",
    "z_deconv2 = tf.nn.softmax(h_deconv2)\n",
    "\n",
    "W_deconv1 = weight_variable([WIDTH2, WIDTH2, DEPTH1, DEPTH2], stddev = STD / (WIDTH2 * WIDTH2 * DEPTH1), name = \"W_deconv1\")\n",
    "z_in1 =   tf.scalar_mul(training_step[1], tf.nn.softmax(h_conv2)  ) \\\n",
    "        + tf.scalar_mul(1 - ((1 - training_step[2]) * (1 - training_step[3])), z_deconv2)\n",
    "h_deconv1 = tf.nn.conv2d_transpose(z_in1, W_deconv1, output_shape=[BATCH_SIZE, OUT1, OUT1, DEPTH1],\n",
    "                                        strides=[1, WIDTH2, WIDTH2, 1], padding='VALID') \n",
    "z_deconv1 = tf.nn.softmax(h_deconv1)\n",
    "\n",
    "W_deconv0 = weight_variable([WIDTH1, WIDTH1, 1, DEPTH1], stddev = STD / (WIDTH1 * WIDTH1), name = \"W_deconv0\")\n",
    "z_in0 =   tf.scalar_mul( training_step[0]     , tf.nn.softmax(h_conv1)   ) \\\n",
    "        + tf.scalar_mul( 1 - ((1 - training_step[1]) * (1 - training_step[2]) * (1 - training_step[3])) , z_deconv1)\n",
    "h_deconv0 = tf.nn.conv2d_transpose(z_in0, W_deconv0, output_shape=[BATCH_SIZE, 32, 32, 1],\n",
    "                                        strides=[1, WIDTH1, WIDTH1, 1], padding='VALID') \n",
    "x_deconv0 = tf.nn.sigmoid(h_deconv0)\n",
    "x_out = tf.reshape(x_deconv0, [-1, 32 * 32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_cnn))\n",
    "#beta = 0\n",
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_cnn)\\\n",
    "#                               + beta * (tf.nn.l2_loss(W_conv1) \\\n",
    "#                                      + tf.nn.l2_loss(W_conv2) \\\n",
    "#                                      + tf.nn.l2_loss(W_out)))\n",
    "\n",
    "#gen_loss1 = tf.reduce_mean(tf.square(x - x_out))\n",
    "#gen_loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=z_conv1, logits=h_deconv1))\n",
    "#tf.reduce_mean(tf.square(z_conv1 - z_deconv1))\n",
    "#gen_loss2 = tf.reduce_mean(tf.square(z_conv2 - z_deconv2))\n",
    "#gen_loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=z_conv2, logits=h_deconv2))\n",
    "\n",
    "gen_loss = tf.reduce_mean(tf.square(x - x_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_train_stage1 = tf.train.AdamOptimizer(3e-5).minimize(classif_loss, var_list = [W_conv1, W_out1])\n",
    "classif_train_stage2 = tf.train.AdamOptimizer(1e-4).minimize(classif_loss, var_list = [W_conv1, W_conv2, W_out2])\n",
    "classif_train_stage3 = tf.train.AdamOptimizer(1e-4).minimize(classif_loss, var_list = [W_conv1, W_conv2, W_conv3, W_out3])\n",
    "\n",
    "gen_train_stage1 = tf.train.AdamOptimizer(1e-4).minimize(gen_loss , var_list = [W_deconv0, W_conv1])\n",
    "gen_train_stage2 = tf.train.AdamOptimizer(1e-4).minimize(gen_loss , var_list = [W_deconv0, W_deconv1, W_conv1, W_conv2])\n",
    "gen_train_stage3 = tf.train.AdamOptimizer(3e-5).minimize(gen_loss , var_list = [W_deconv0, W_deconv1, W_deconv2, W_conv1, W_conv2, W_conv3])\n",
    "gen_train_stage4 = tf.train.AdamOptimizer(3e-5).minimize(gen_loss , var_list = [W_deconv0, W_deconv1, W_deconv2, W_deconv3, W_conv1, W_conv2, W_conv3])\n",
    "#gen_train_step = tf.train.AdamOptimizer(1e-4).minimize(gen_loss , var_list = [W_deconv1, b_deconv1, W_deconv0, b_deconv0])\n",
    "\n",
    "\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_cnn, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "gen_accuracy = tf.sqrt(tf.reduce_mean(tf.square(x - x_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = 1\n",
    "if turn ==1:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train session Interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [2,2,32,64] rhs shape= [2,2,32,16]\n\t [[Node: save/Assign_26 = Assign[T=DT_FLOAT, _class=[\"loc:@W_conv3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_conv3, save/RestoreV2_26)]]\n\nCaused by op u'save/Assign_26', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-18da33d742f9>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 439, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [2,2,32,64] rhs shape= [2,2,32,16]\n\t [[Node: save/Assign_26 = Assign[T=DT_FLOAT, _class=[\"loc:@W_conv3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_conv3, save/RestoreV2_26)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-31b67403ae22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [2,2,32,64] rhs shape= [2,2,32,16]\n\t [[Node: save/Assign_26 = Assign[T=DT_FLOAT, _class=[\"loc:@W_conv3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_conv3, save/RestoreV2_26)]]\n\nCaused by op u'save/Assign_26', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-18da33d742f9>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 439, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [2,2,32,64] rhs shape= [2,2,32,16]\n\t [[Node: save/Assign_26 = Assign[T=DT_FLOAT, _class=[\"loc:@W_conv3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_conv3, save/RestoreV2_26)]]\n"
     ]
    }
   ],
   "source": [
    "#with tf.Session() as sess:\n",
    "if not os.path.isfile(\"models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt.index\"):\n",
    "    for rep in range(200): #120\n",
    "        for i in range(20000): #2400000\n",
    "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "            if i % 10 == 0:\n",
    "                train_accuracy     = accuracy.eval(    feed_dict = {x: mnist_reshape_32(batch[0]),\\\n",
    "                                                                    y_: batch[1],\\\n",
    "                                                                    training_step : [0, 0, 0, 1],\\\n",
    "                                                                    keep_prob : 1})\n",
    "                train_gen_accuracy = gen_accuracy.eval(feed_dict = {x: mnist_reshape_32(batch[0]),\\\n",
    "                                                                    y_: batch[1],\\\n",
    "                                                                    training_step : [0, 0, 0, 1],\\\n",
    "                                                                    keep_prob : 1}) \n",
    "                sys.stdout.write('\\rstep %d, training accuracy : %.2f, generative avg error : %.4f' % (i + rep * 20000, \\\n",
    "                                                                                                       train_accuracy, \\\n",
    "                                                                                                       train_gen_accuracy))\n",
    "                sys.stdout.flush()\n",
    "            if i % 4 == 0:    \n",
    "                classif_train_stage1.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                                   y_: batch[1],\\\n",
    "                                                   training_step : [1, 0, 0, 0],\\\n",
    "                                                    keep_prob : .5}) \n",
    "                gen_train_stage1.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                               training_step : [1, 0, 0, 0],\\\n",
    "                                                keep_prob : .5}) \n",
    "            elif i % 4 == 1:\n",
    "                classif_train_stage2.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                                   y_: batch[1],\\\n",
    "                                                   training_step : [0, 1, 0, 0],\\\n",
    "                                                    keep_prob : .5}) \n",
    "                gen_train_stage2.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                               training_step : [0, 1, 0, 0],\\\n",
    "                                                keep_prob : .5\n",
    "                                               }) \n",
    "            elif i % 4 == 2:\n",
    "                classif_train_stage3.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                                   y_: batch[1],\\\n",
    "                                                   training_step : [0, 0, 1, 0],\\\n",
    "                                                    keep_prob : .5}) \n",
    "                gen_train_stage3.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                               training_step : [0, 0, 1, 0],\\\n",
    "                                                keep_prob : .5}) \n",
    "            else:\n",
    "                classif_train_stage3.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                                   y_: batch[1],\\\n",
    "                                                   training_step : [0, 0, 0, 1],\\\n",
    "                                                    keep_prob : .5}) \n",
    "                gen_train_stage4.run(feed_dict={x: mnist_reshape_32(batch[0]),\\\n",
    "                                               training_step : [0, 0, 0, 1],\\\n",
    "                                                keep_prob : .5}) \n",
    "        saver.save(sess, \"models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt\")\n",
    "else:\n",
    "    saver.restore(sess,'models/deep-MNIST-3-couches-incremental-optim-sans-biais-INTERLEAVED-RELU-DROP-64.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes** :\n",
    "  - Avec connectivité totale des couches forward et bakward (ladder connectivity --> effet de regularisation)\n",
    "  - Connectivité selon training_step (app incremental)--> l'erreur de reconstruction stagne à 0.1\n",
    "  - Connectivité selon training_step (app incremental) avec back-prop sur les liens entrants (discriminatifs)\n",
    "    --> l'erreur de reconstruction descend à 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images),\\\n",
    "                                                    y_: mnist.test.labels,\\\n",
    "                                                    training_step : [1, 0, 0, 0],\\\n",
    "                                                                    keep_prob : 1}))\n",
    "print('test generative accuracy %g' % gen_accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images)[:50,:],\\\n",
    "                                                                   y_: mnist.test.labels[:50,:],\\\n",
    "                                                                   training_step : [1, 0, 0, 0],\\\n",
    "                                                                    keep_prob : 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images),\\\n",
    "                                                    y_: mnist.test.labels,\\\n",
    "                                                    training_step : [0, 1, 0, 0],\\\n",
    "                                                                    keep_prob : 1}))\n",
    "print('test generative accuracy %g' % gen_accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images)[:50,:],\\\n",
    "                                                                   y_: mnist.test.labels[:50,:],\\\n",
    "                                                                   training_step : [0, 1, 0, 0],\\\n",
    "                                                                    keep_prob : 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images),\\\n",
    "                                                    y_: mnist.test.labels,\\\n",
    "                                                    training_step : [0, 0, 1, 0],\\\n",
    "                                                                    keep_prob : 1}))\n",
    "print('test generative accuracy %g' % gen_accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images)[:50,:],\\\n",
    "                                                                   y_: mnist.test.labels[:50,:],\\\n",
    "                                                                   training_step : [0, 0, 1, 0],\\\n",
    "                                                                    keep_prob : 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images),\\\n",
    "                                                    y_: mnist.test.labels,\\\n",
    "                                                    training_step : [0, 0, 0, 1],\\\n",
    "                                                                    keep_prob : 1}))\n",
    "print('test generative accuracy %g' % gen_accuracy.eval(feed_dict={x: mnist_reshape_32(mnist.test.images)[:50,:],\\\n",
    "                                                                   y_: mnist.test.labels[:50,:],\\\n",
    "                                                                   training_step : [0, 0, 0, 1],\\\n",
    "                                                                    keep_prob : 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step_test = [0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = mnist.test.next_batch(BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sess.run(z_conv1, {x: mnist_reshape_32(batch[0][0]), training_step : training_step_test, keep_prob : 1})\n",
    "print test.shape\n",
    "figure()\n",
    "for k in range(30):        \n",
    "    test = sess.run(z_conv1, {x: mnist_reshape_32(batch[0][k]), training_step : training_step_test, keep_prob : 1})\n",
    "    for i in range(OUT1):\n",
    "        for j in range(OUT1):\n",
    "            plot(test[0,i,j,:])\n",
    "\n",
    "test = sess.run(z_conv2, {x: mnist_reshape_32(batch[0][0]), training_step : training_step_test, keep_prob : 1})\n",
    "print test.shape\n",
    "figure()\n",
    "for k in range(30):        \n",
    "    test = sess.run(z_conv2, {x: mnist_reshape_32(batch[0][k]), training_step : training_step_test, keep_prob : 1})\n",
    "    for i in range(OUT2):\n",
    "        for j in range(OUT2):\n",
    "            plot(test[0,i,j,:])\n",
    "\n",
    "test = sess.run(z_conv3, {x: mnist_reshape_32(batch[0][0]), training_step : training_step_test, keep_prob : 1})\n",
    "print test.shape\n",
    "figure()\n",
    "for k in range(30):        \n",
    "    test = sess.run(z_conv3, {x: mnist_reshape_32(batch[0][k]), training_step : training_step_test, keep_prob : 1})\n",
    "    for i in range(OUT3):\n",
    "        for j in range(OUT3):\n",
    "            plot(test[0,i,j,:])\n",
    "\n",
    "test = sess.run(y_cnn, {x: mnist_reshape_32(batch[0][0]), training_step : training_step_test, keep_prob : 1})\n",
    "print test.shape\n",
    "figure()\n",
    "for k in range(10):        \n",
    "    test = sess.run(y_cnn, {x: mnist_reshape_32(batch[0][k]), training_step : training_step_test, keep_prob : 1})\n",
    "    plot(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    figure()\n",
    "    subplot(1,2,1)\n",
    "    imshow(mnist_reshape_32(batch[0][k,:]).reshape((32,32)), cmap = 'gray_r')\n",
    "    subplot(1,2,2)\n",
    "    imshow(x_out.eval(feed_dict={x: mnist_reshape_32(batch[0]), training_step : training_step_test, keep_prob : 1})[k,:].reshape((32,32)), cmap = 'gray_r')\n",
    "    print(tf.argmax(y_cnn, 1).eval(feed_dict={x: mnist_reshape_32(batch[0][k]), training_step : training_step_test, keep_prob : 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print W_deconv0.eval().shape\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    imshow(W_deconv0.eval()[:,:,0,i].reshape((4,4)), cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print OUT1, OUT1, DEPTH1\n",
    "print OUT2, OUT2, DEPTH2\n",
    "print OUT3, OUT3, DEPTH3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saccades algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x_coords(x, liste_coords, aff = False):\n",
    "    # coord in 0..7\n",
    "    assert x.shape == ((32 * 32,))\n",
    "    image_in = x.reshape((32, 32)) \n",
    "    image_out = np.zeros((32, 32))\n",
    "    if aff:\n",
    "        image_out += .3\n",
    "    for u in liste_coords:\n",
    "        patch = image_in[int(u[0] * 4) : int(u[0] * 4) + 4, int(u[1] * 4) : int(u[1] * 4) + 4]\n",
    "        image_out[int(u[0] * 4) : int(u[0] * 4) + 4, int(u[1] * 4) : int(u[1] * 4) + 4] = patch\n",
    "    return image_out.reshape((32 * 32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''liste_coords = [(2, 2), (2, 3), (2, 4), (2, 5),\\\n",
    "                (3, 2), (3, 3), (3, 4), (3, 5),\\\n",
    "                (4, 2), (4, 3), (4, 4), (4, 5),\\\n",
    "                (5, 2), (5, 3), (5, 4), (5, 5)]'''\n",
    "liste_coords = [(2.5, 2.5), (2.5, 3.5), (2.5, 4.5),\\\n",
    "                (3.5, 2.5), (3.5, 3.5), (3.5, 4.5),\\\n",
    "                (4.5, 2.5), (4.5, 3.5), (4.5, 4.5)]\n",
    "batch_reduced = np.zeros((BATCH_SIZE, 1024))\n",
    "for k in range(BATCH_SIZE):\n",
    "    x_reduced = gen_x_coords(mnist_reshape_32(batch[0])[k], liste_coords)\n",
    "    batch_reduced[k,:] = x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    y_real = argmax(batch[1][k])\n",
    "    y_infer = tf.argmax(y_cnn, 1).eval(feed_dict={x: batch_reduced, training_step : training_step_test, keep_prob : 1})[k]\n",
    "    figure()\n",
    "    subplot(1,2,1)\n",
    "    imshow(batch_reduced[k,:].reshape((32,32)), cmap = 'gray_r')\n",
    "    title('Real class : ' + str(y_real))\n",
    "    subplot(1,2,2)\n",
    "    imshow(x_out.eval(feed_dict={x: batch_reduced, training_step : training_step_test, keep_prob : 1})[k,:].reshape((32,32)), cmap = 'gray_r')\n",
    "    title('Inferred class : ' + str(y_infer))\n",
    "    print(y_real,\\\n",
    "          tf.nn.softmax(y_cnn.eval(feed_dict={x: batch_reduced, training_step : training_step_test, keep_prob : 1})[k]).eval()[y_infer],\\\n",
    "          y_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = .5\n",
    "TURN_MAX = 49 #12\n",
    "THRESHOLD = .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "# Data\n",
    "batch_X = mnist_reshape_32(batch[0])\n",
    "X = batch_X[k]\n",
    "batch_X_reduced = np.zeros((BATCH_SIZE, 32 * 32))\n",
    "batch_classif = batch[1]\n",
    "# Initialization\n",
    "U = {}\n",
    "for i in range(OUT1-1):\n",
    "    for j in range(OUT1-1):\n",
    "        U[(i,j)] = 1\n",
    "u = (3, 3) #(2 + np.random.randint(4), 2 + np.random.randint(4))\n",
    "liste_u = np.array([u])\n",
    "U.pop(u)\n",
    "# fill batch_X_reduced\n",
    "X_reduced = gen_x_coords(batch_X[k], liste_u + OFFSET)\n",
    "batch_X_reduced[k] = X_reduced\n",
    "mem_X_reduced = [X_reduced]\n",
    "# prior\n",
    "q = tf.nn.softmax(y_cnn.eval(feed_dict={x: X_reduced.reshape(1,1024), training_step : training_step_test, keep_prob : 1})).eval()[0]\n",
    "mem_q = np.zeros((OUT1 * OUT1 + 1, 10))\n",
    "mem_q[0, :] = [.1] * 10\n",
    "mem_q[1, :] = q\n",
    "current_hyp = np.where(np.random.multinomial(1,q) == 1)[0][0]\n",
    "max_hyp = argmax(q) \n",
    "#current_hyp = \n",
    "#print q, current_hyp, argmax(batch_classif[k])\n",
    "turn = 1\n",
    "while turn < TURN_MAX and q[max_hyp] < THRESHOLD: #OUT1 * OUT1):\n",
    "    print turn, str(argmax(batch_classif[k])), max_hyp, q[max_hyp]\n",
    "    f = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    X_reduced_aff = gen_x_coords(batch_X[k], liste_u + OFFSET, aff = True)\n",
    "    plt.imshow(X_reduced_aff.reshape((32,32)), cmap = 'gray_r', vmin = 0, vmax = 1)\n",
    "    title('saccade #' +  str(turn) + ', real class = ' + str(argmax(batch_classif[k])))\n",
    "    # affichage\n",
    "    i_N = 4 * (liste_u[-1][0] + OFFSET) - .5\n",
    "    i_S = 4 * (liste_u[-1][0] + OFFSET) + 3.5\n",
    "    j_W = 4 * (liste_u[-1][1] + OFFSET) - .5\n",
    "    j_E = 4 * (liste_u[-1][1] + OFFSET) + 3.5\n",
    "\n",
    "    plt.plot([j_W, j_E], [i_N, i_N],'r')\n",
    "    plt.plot([j_W, j_W], [i_N, i_S], 'r')\n",
    "    plt.plot([j_W, j_E], [i_S, i_S], 'r')\n",
    "    plt.plot([j_E, j_E], [i_N, i_S], 'r')\n",
    "    xlim([-0.5, 31.5])\n",
    "    ylim([-0.5, 31.5])\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    X_generated = x_out.eval(feed_dict={x: batch_X_reduced, \\\n",
    "                                        training_step : training_step_test, keep_prob : 1})[k,:]\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(X_generated.reshape((32,32)), cmap = 'gray_r', vmin = 0, vmax = 1)\n",
    "    plt.title('hyp = ' + str(max_hyp) + ' (' + str(q[max_hyp]) + ')')\n",
    "    gen_images = {}\n",
    "    batch_X_patch = np.zeros((len(U), 32 * 32))\n",
    "    dict_u = {}\n",
    "    for i, u_gen in enumerate(U):\n",
    "        dict_u[i] = u_gen\n",
    "        batch_X_patch[i,:] = gen_x_coords(X_generated, [(u_gen[0] + OFFSET, u_gen[1] + OFFSET)])\n",
    "    q_gen = tf.nn.softmax(y_cnn.eval(feed_dict={x: batch_X_patch,\\\n",
    "                                                training_step : training_step_test, keep_prob : 1})).eval()\n",
    "    i_max = np.where(q_gen[:, current_hyp] == max(q_gen[:, current_hyp]))[0][0]\n",
    "    u_max = dict_u[i_max]\n",
    "    \n",
    "    # affichage\n",
    "    i_N = 4 * (u_max[0] + OFFSET) - .5\n",
    "    i_S = 4 * (u_max[0] + OFFSET) + 3.5\n",
    "    j_W = 4 * (u_max[1] + OFFSET) - .5\n",
    "    j_E = 4 * (u_max[1] + OFFSET) + 3.5\n",
    "\n",
    "    plt.plot([j_W, j_E], [i_N, i_N],'r:')\n",
    "    plt.plot([j_W, j_W], [i_N, i_S], 'r:')\n",
    "    plt.plot([j_W, j_E], [i_S, i_S], 'r:')\n",
    "    plt.plot([j_E, j_E], [i_N, i_S], 'r:')\n",
    "    \n",
    "    i_prec = 4 * (liste_u[-1][0] + .5) + 1.5\n",
    "    j_prec = 4 * (liste_u[-1][1] + .5) + 1.5\n",
    "    i_new = 4 * (u_max[0] + .5) + 1.5\n",
    "    j_new = 4 * (u_max[1] + .5) + 1.5\n",
    "    \n",
    "    plt.plot([j_prec, j_new], [i_prec, i_new], 'r')\n",
    "    plt.plot([j_prec], [i_prec], 'rx')\n",
    "    plt.plot([j_new], [i_new], 'rx')\n",
    "    \n",
    "    #plt.gca().invert_xaxis()\n",
    "    xlim([-0.5, 31.5])\n",
    "    ylim([-0.5, 31.5])\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    liste_u = np.append(liste_u, np.array([u_max]), axis = 0)\n",
    "    #print liste_u\n",
    "    U.pop(u_max)\n",
    "    X_reduced = gen_x_coords(batch_X[k], liste_u + OFFSET)\n",
    "    batch_X_reduced[k] = X_reduced\n",
    "    mem_X_reduced += [X_reduced]\n",
    "    q = tf.nn.softmax(y_cnn.eval(feed_dict={x: X_reduced.reshape(1,1024), training_step : training_step_test, keep_prob : 1})).eval()[0]\n",
    "    q = q / np.sum(q)\n",
    "    turn += 1\n",
    "    mem_q[turn, :] = q\n",
    "    #print q, argmax(q), argmax(batch_classif[k])\n",
    "    current_hyp = np.where(np.random.multinomial(1,q) == 1)[0][0] #\n",
    "    max_hyp = argmax(q) #\n",
    "    \n",
    "current_hyp = argmax(q)\n",
    "print  'Nb saccades :', turn, 'real_class = ', str(argmax(batch_classif[k])), 'estim class = ', str(current_hyp) \n",
    "f = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "X_reduced_aff = gen_x_coords(batch_X[k], liste_u + OFFSET, aff = True)\n",
    "plt.imshow(X_reduced_aff.reshape((32,32)), cmap = 'gray_r', vmin = 0, vmax = 1)\n",
    "title('saccade #' +  str(turn) + ', real class = ' + str(argmax(batch_classif[k])))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_generated.reshape((32,32)), cmap = 'gray_r', vmin = 0, vmax = 1)\n",
    "plt.title('final hyp = ' + str(current_hyp) + ' (' + str(q[current_hyp]) + ')')\n",
    "\n",
    "f = figure()\n",
    "p = plot(mem_q[:turn + 1,:])\n",
    "xlim([0, TURN_MAX])\n",
    "ylim([0, 1])\n",
    "#handles, labels = f.  get_legend_handles_labels()\n",
    "title('Posterior distribution')\n",
    "f.legend(p, range(10))\n",
    "xlabel('saccade #')\n",
    "#image = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mem_mem_q = []\n",
    "mem_liste_u = []\n",
    "mem_classif = []\n",
    "mem_class = []\n",
    "mem_final_hyp = []\n",
    "mem_turn = []\n",
    "for num_batch in range(20):\n",
    "    batch = mnist.test.next_batch(BATCH_SIZE) \n",
    "    for k in range(BATCH_SIZE):\n",
    "        print num_batch * 20 + k\n",
    "        batch_X = mnist_reshape_32(batch[0])\n",
    "        X = batch_X[k]\n",
    "        batch_X_reduced = np.zeros((BATCH_SIZE, 32 * 32))\n",
    "        batch_classif = batch[1]\n",
    "        # Initialization\n",
    "        U = {}\n",
    "        for i in range(OUT1-1):\n",
    "            for j in range(OUT1-1):\n",
    "                U[(i,j)] = 1\n",
    "        u = (3, 3) #(2 + np.random.randint(4), 2 + np.random.randint(4))\n",
    "        liste_u = np.array([u])\n",
    "        U.pop(u)\n",
    "        # fill batch_X_reduced\n",
    "        X_reduced = gen_x_coords(batch_X[k], liste_u + OFFSET)\n",
    "        batch_X_reduced[k] = X_reduced\n",
    "        mem_X_reduced = [X_reduced]\n",
    "        # prior\n",
    "        mem_q = np.zeros((OUT1 * OUT1 + 1, 10))\n",
    "        mem_q[0, :] = [.1] * 10\n",
    "        q = tf.nn.softmax(y_cnn.eval(feed_dict={x: X_reduced.reshape(1,1024), training_step : training_step_test})).eval()[0]\n",
    "        mem_q[1, :] = q\n",
    "        current_hyp = np.where(np.random.multinomial(1,q) == 1)[0][0]\n",
    "        max_hyp = argmax(q) \n",
    "        #current_hyp = \n",
    "        #print q, current_hyp, argmax(batch_classif[k])\n",
    "        turn = 1\n",
    "        while turn < TURN_MAX and q[max_hyp] < THRESHOLD: #OUT1 * OUT1):\n",
    "\n",
    "            X_generated = x_out.eval(feed_dict={x: batch_X_reduced, \\\n",
    "                                                training_step : training_step_test})[k,:]\n",
    "\n",
    "            batch_X_patch = np.zeros((len(U), 32 * 32))\n",
    "            dict_u = {}\n",
    "            for i, u_gen in enumerate(U):\n",
    "                dict_u[i] = u_gen\n",
    "                batch_X_patch[i,:] = gen_x_coords(X_generated, [(u_gen[0] + OFFSET, u_gen[1] + OFFSET)])\n",
    "            q_gen = tf.nn.softmax(y_cnn.eval(feed_dict={x: batch_X_patch,\\\n",
    "                                                        training_step : training_step_test})).eval()\n",
    "            i_max = np.where(q_gen[:, current_hyp] == max(q_gen[:, current_hyp]))[0][0]\n",
    "            u_max = dict_u[i_max]\n",
    "\n",
    "            liste_u = np.append(liste_u, np.array([u_max]), axis = 0)\n",
    "            #print liste_u\n",
    "            U.pop(u_max)\n",
    "            X_reduced = gen_x_coords(batch_X[k], liste_u + OFFSET)\n",
    "            batch_X_reduced[k] = X_reduced\n",
    "            mem_X_reduced += [X_reduced]\n",
    "            q = tf.nn.softmax(y_cnn.eval(feed_dict={x: X_reduced.reshape(1,1024), training_step : training_step_test})).eval()[0]\n",
    "            turn += 1\n",
    "            mem_q[turn, :] = q\n",
    "            #print q, argmax(q), argmax(batch_classif[k])\n",
    "            current_hyp = np.where(np.random.multinomial(1,q) == 1)[0][0] #\n",
    "            max_hyp = argmax(q) #\n",
    "\n",
    "        current_hyp = argmax(q)\n",
    "        print  'Nb saccades :', turn, ', real_class = ', str(argmax(batch_classif[k])), 'estim class = ', str(current_hyp) \n",
    "\n",
    "\n",
    "        #f = figure()\n",
    "        #p = plot(mem_q[:turn + 1,:])\n",
    "        #xlim([0, TURN_MAX])\n",
    "        #ylim([0, 1])\n",
    "        #title('Posterior distribution')\n",
    "        #f.legend(p, range(10))\n",
    "        #xlabel('saccade #')\n",
    "\n",
    "        mem_mem_q += [mem_q] \n",
    "        mem_liste_u += [mem_liste_u] \n",
    "        mem_classif += [argmax(batch_classif[k]) == current_hyp]\n",
    "        mem_turn += [turn]\n",
    "        mem_class += [argmax(batch_classif[k])]\n",
    "        mem_final_hyp += [current_hyp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean(mem_turn)\n",
    "print mean(mem_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean(mem_turn)\n",
    "print mean(mem_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_turn_49 = np.copy(mem_turn)\n",
    "mem_classif_49 = np.copy(mem_classif)\n",
    "h = hist(mem_turn_49, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im = np.zeros(1024)\n",
    "for liste_u in mem_liste_u:\n",
    "    tmp = gen_x_coords(np.ones(1024), np.array(liste_u) + OFFSET)\n",
    "    im += tmp\n",
    "imshow(im.reshape(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 643 saccades (max 49) : TURN MAX\n",
    "mean(mem_turn) : 19.6562986003\n",
    "mean(mem_classif) : 0.858475894246\n",
    "    \n",
    "# Idea : the model hs learned that theinformation is in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean(mem_turn)\n",
    "print mean(mem_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
